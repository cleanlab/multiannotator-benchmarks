{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264fb0bf",
   "metadata": {},
   "source": [
    "# This notebook runs a train and eval loop on models with improving consensus labels over each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from utils.model_training import train_models\n",
    "from utils.model_training import sum_xval_folds\n",
    "from utils.data_loading import get_annotator_labels\n",
    "from utils.data_loading import drop_and_distribute\n",
    "from utils.data_loading import get_and_save_consensus_labels\n",
    "from utils.data_loading import get_ground_truth_data_matched\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # current date and time\n",
    "experiment_folder = \"experiment_\" + str(int(now.timestamp()))\n",
    "dirName = './data/experiments/' + experiment_folder\n",
    "\n",
    "if not os.path.exists(dirName):\n",
    "    os.makedirs(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , dirName ,  \" already exists\")\n",
    "\n",
    "print(f'Experiment saved in {dirName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a6e28",
   "metadata": {},
   "source": [
    "## Dropout data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cifar10h dataset and dropout information from it\n",
    "cifar10_infolder = './data/cifar-10h/cifar10h-raw.csv' #c10h raw data folder\n",
    "max_annotations = 5\n",
    "\n",
    "c10h_labels, c10h_true_labels, c10h_true_images = get_annotator_labels(cifar10_infolder)\n",
    "c10h_labels = drop_and_distribute(c10h_labels, max_annotations)\n",
    "\n",
    "# save c10h_results\n",
    "cifar10_labels_folder = f\"{dirName}/todelete_c10h_labels_range_{max_annotations}.npy\"\n",
    "cifar10_true_labels_folder = f\"{dirName}/todelete_c10h_true_labels_range_{max_annotations}.npy\"\n",
    "np.save(cifar10_labels_folder, c10h_labels)\n",
    "np.save(cifar10_true_labels_folder, c10h_true_labels)\n",
    "\n",
    "# Generate and save original consensus labels\n",
    "consensus_outfolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_0.csv' #output folder for consensus labels\n",
    "consensus_labels = get_and_save_consensus_labels(c10h_labels, c10h_true_labels, consensus_outfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3ebe3",
   "metadata": {},
   "source": [
    "## Train models through loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consensus labels and train model on them\n",
    "models = [\n",
    "    \"resnet18\",\n",
    "    \"swin_base_patch4_window7_224\"\n",
    "]\n",
    "\n",
    "train_args = {\n",
    "    \"num_cv_folds\": 5, \n",
    "    \"verbose\": 1, \n",
    "    \"epochs\": 1, \n",
    "    \"holdout_frac\": 0.2, \n",
    "    \"time_limit\": 60, \n",
    "    \"random_state\": 123\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and retrain model on better pred-probs\n",
    "NUM_MODEL_RETRAINS = 3\n",
    "\n",
    "for i in range(NUM_MODEL_RETRAINS):\n",
    "    for model in models:\n",
    "        # Get folders\n",
    "        if i == 0:\n",
    "            consensus_infolder = consensus_outfolder\n",
    "        else:\n",
    "            consensus_infolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i-1}_{model}.csv'\n",
    "        model_results_folder = f'{dirName}/todelete_cifar10_consensus_range_{max_annotations}_{i}' # + [model_type]\n",
    "        consensus_outfolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i}_{model}.csv'\n",
    "\n",
    "        print(f'--INFO {i}_{model}--')\n",
    "        print('Loading consensus from', consensus_infolder)\n",
    "        print('Saving consensus to', consensus_outfolder)\n",
    "        print('Saving model results to', model_results_folder)\n",
    "        print('---------------------')\n",
    "        \n",
    "        # Train model\n",
    "        train_models([model], consensus_infolder, model_results_folder, **train_args)\n",
    "        pred_probs, labels , true_labels, images = sum_xval_folds([model], model_results_folder, **train_args)\n",
    "\n",
    "        # Generate and save consensus labels\n",
    "        _ = get_and_save_consensus_labels(c10h_labels, c10h_true_labels, consensus_outfolder, pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e62b2",
   "metadata": {},
   "source": [
    "## Compute accuracy of model based on Accuracy (labels vs true labels) by itter after folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_noisy_vs_true_labels = (consensus_labels['label'].values == c10h_true_labels).mean()\n",
    "print(f\"Accuracy ORIGINAL (consensus labels vs true labels): {acc_noisy_vs_true_labels}\\n\")\n",
    "\n",
    "for model in models:\n",
    "    for i in range(NUM_MODEL_RETRAINS):\n",
    "        \n",
    "        # Get folders\n",
    "        if i == 0:\n",
    "            consensus_infolder = consensus_outfolder\n",
    "        else:\n",
    "            consensus_infolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i-1}_{model}.csv'\n",
    "        model_results_folder = f'{dirName}/todelete_cifar10_consensus_range_{max_annotations}_{i}' # + [model_type]\n",
    "    \n",
    "        print(f'--{model} iter{i}--')\n",
    "        \n",
    "        out_subfolder = f\"{model_results_folder}_{model}/\"\n",
    "        pred_probs = np.load(out_subfolder + \"pred_probs.npy\")\n",
    "        labels = np.load(out_subfolder + \"labels.npy\") # remember that this is the noisy labels (s)\n",
    "        images = np.load(out_subfolder + \"images.npy\", allow_pickle=True)\n",
    "        true_labels = np.load(out_subfolder + \"true_labels.npy\")\n",
    "\n",
    "        # check the accuracy\n",
    "        acc_labels = (pred_probs.argmax(axis=1) == labels).mean() # noisy labels (s)\n",
    "        acc_true_labels = (pred_probs.argmax(axis=1) == true_labels).mean() # true labels (y)    \n",
    "        acc_noisy_vs_true_labels = (labels == true_labels).mean()\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Accuracy (argmax pred vs labels)                 : {acc_labels}\")\n",
    "        print(f\"  Accuracy (argmax pred vs true labels)            : {acc_true_labels}\")\n",
    "        print(f\"  Accuracy (consensus labels vs true labels)       : {acc_noisy_vs_true_labels}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
