{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3aea21",
   "metadata": {},
   "source": [
    "# CL for multi-annotator data [cifar10h] [benchmarking]\n",
    "- This notebook uses the results from the model_train_pred notebook to evaluate model performance. Also evaluates general multi-annotator dataset health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fee266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.multiannotator import get_label_quality_multiannotator, get_multiannotator_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cleanlab\n",
    "from cleanlab.rank import get_label_quality_scores, get_label_quality_ensemble_scores\n",
    "from cleanlab.internal.label_quality_utils import get_normalized_entropy\n",
    "from cleanlab.filter import find_label_issues\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, precision_recall_curve, roc_curve, accuracy_score, log_loss\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from utils.eval_metrics import lift_at_k\n",
    "from utils.active_learning_scores import least_confidence\n",
    "# experimental version of label quality ensemble scores with additional weighting schemes\n",
    "from utils.label_quality_ensemble_scores_experimental import get_label_quality_ensemble_scores_experimental\n",
    "\n",
    "path = os.getcwd()\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69495f73",
   "metadata": {},
   "source": [
    "### Load/Analyze Cifar10h Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b69b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this line throws an error, make sure you correctly downloaded and unzipped cifar10h-raw data\n",
    "\n",
    "pred_probs_multiannotator = np.load('./data/cifar10h/cifar10h-probs.npy')\n",
    "df = pd.read_csv('./data/cifar10h/cifar10h-raw.csv')\n",
    "df = df[df.cifar10_test_test_idx != -99999] # dropping all attention check trials\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c39b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e4d8b",
   "metadata": {},
   "source": [
    "#### restructure dataset information\n",
    "- num_datapoints (N), num_annotators (M)\n",
    "- hlabels: (N,M)\n",
    "- hlabels_error_mask: (N,M) where True=error\n",
    "- hannotator_mask: (N,M) where True=annotator x anotated that\n",
    "- htrue_labels: (K=10,000,) # indexed same way as cifar10\n",
    "- htrue_images: (K=10,000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c869ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotations per annotator in numpy array (rows = annotations, cols = annotators)\n",
    "\n",
    "num_datapoints = df['cifar10_test_test_idx'].max() + 1\n",
    "num_annotators = df['annotator_id'].max() + 1\n",
    "\n",
    "hlabels = np.full((num_datapoints, num_annotators), np.nan) # all annotator labels np.full([height, width, 9], np.nan)\n",
    "hlabels_error_mask = np.zeros((num_datapoints, num_annotators), dtype=bool) # mask of annotator errors\n",
    "hannotator_mask = np.zeros((num_datapoints, num_annotators), dtype=bool) # mask of what each person annotated\n",
    "\n",
    "print(hlabels.shape, hlabels.sum(), hlabels_error_mask.shape, hlabels_error_mask.sum(), hannotator_mask.shape, hannotator_mask.sum())\n",
    "\n",
    "for annotator_id in range(num_annotators):\n",
    "    adf = df[df.annotator_id == annotator_id] # 200 annotations per annotator\n",
    "    annotations_idx = adf['cifar10_test_test_idx'].values\n",
    "    annotations = adf['chosen_label'].values\n",
    "    errors = adf['correct_guess'].values\n",
    "    \n",
    "    hlabels[annotations_idx, annotator_id] = annotations\n",
    "    hlabels_error_mask[annotations_idx, annotator_id] = errors\n",
    "    hannotator_mask[annotations_idx, annotator_id] = True\n",
    "\n",
    "print(hlabels.shape, hlabels.sum(), hlabels_error_mask.shape, hlabels_error_mask.sum(), hannotator_mask.shape, hannotator_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f15235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true labels as numpy array (rows = true labels,) and true images\n",
    "\n",
    "htrue_labels = np.zeros((num_datapoints, ))\n",
    "htrue_images = np.empty((num_datapoints, ) ,dtype=object)\n",
    "\n",
    "idx_to_label = \\\n",
    "[(idx,label,image) for idx,label,image in zip(df['cifar10_test_test_idx'],df['true_label'],df['image_filename'])]\n",
    "idx_to_label = list(set(idx_to_label))\n",
    "\n",
    "idx = [idx_to_label[0] for idx_to_label in idx_to_label]\n",
    "true_label = [idx_to_label[1] for idx_to_label in idx_to_label]\n",
    "htrue_image = [idx_to_label[2] for idx_to_label in idx_to_label]\n",
    "\n",
    "htrue_labels[idx] = true_label\n",
    "htrue_images[idx] = htrue_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def07fd6",
   "metadata": {},
   "source": [
    "#### get accuracy of individual annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of individual annotators\n",
    "def plt_annotator_accuracy(labels_error_mask, annotator_mask):\n",
    "    annotator_accuracy = labels_error_mask.sum(axis=0) / annotator_mask.sum(axis=0)\n",
    "    plt.boxplot(annotator_accuracy)\n",
    "    plt.show()\n",
    "\n",
    "    df_describe = pd.DataFrame(annotator_accuracy, columns=['score'])\n",
    "    return df_describe\n",
    "\n",
    "df_describe = plt_annotator_accuracy(hlabels_error_mask, hannotator_mask)\n",
    "df_describe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268d90f",
   "metadata": {},
   "source": [
    "#### get accuracy of consensus labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b837561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the consensus_labels\n",
    "# TODO: conditional based on consensus_method, consensus_method can be a List[str], add dawid-skene\n",
    "def get_consensus_labels(labels_multiannotator, pred_probs):\n",
    "    mode_labels_multiannotator = labels_multiannotator.mode(axis=1)\n",
    "    consensus_labels = []\n",
    "    for i in range(len(mode_labels_multiannotator)):\n",
    "        consensus_labels.append( int(mode_labels_multiannotator.iloc[i][pred_probs[i][mode_labels_multiannotator.iloc[i].dropna().astype(int).to_numpy()\n",
    "                    ].argmax()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    return np.array(consensus_labels)\n",
    "\n",
    "def get_consensus_accuracy_report(labels, true_labels, annotator_mask, pred_probs_multiannotator):\n",
    "    labels_multiannotator = pd.DataFrame(labels)\n",
    "    consensus_labels = get_consensus_labels(labels_multiannotator, pred_probs_multiannotator)\n",
    "    correct_consensus = (true_labels == consensus_labels) + 0\n",
    "    all_consensus = [1] * len(true_labels)\n",
    "    correct_consensus.sum() / len(correct_consensus)\n",
    "\n",
    "    num_annotators_per_example = annotator_mask.sum(axis=1)\n",
    "\n",
    "    consensus_accuracy = pd.DataFrame(zip(correct_consensus, num_annotators_per_example,all_consensus), columns=['consense','num_a','total_seen'])\n",
    "    consensus_accuracy = consensus_accuracy.groupby('num_a')[[\"consense\", \"total_seen\"]].sum().reset_index()\n",
    "    consensus_accuracy['consensus_acc'] = consensus_accuracy['consense'] / consensus_accuracy['total_seen']\n",
    "    return consensus_accuracy\n",
    "\n",
    "# per example, number of annotators that agree with consensus label (% agreement = x with lower confidence bound = confidence interval for true proportion of annotators that greed (jonas share))\n",
    "# plot accuracy of consensus label given number of annotators that agree\n",
    "# if acc to num annotators and num annotator agreement % then we perform best\n",
    "\n",
    "def plot_labels_multiannotator(labels, true_labels, pred_probs_multiannotator=None):\n",
    "    labels_multiannotator = pd.DataFrame(labels)\n",
    "    \n",
    "    if pred_probs_multiannotator is None:\n",
    "        pred_counts = labels.sum(axis=1)\n",
    "        pred_probs_multiannotator = labels / pred_counts[:,np.newaxis]\n",
    "\n",
    "    consensus_labels = get_consensus_labels(labels_multiannotator, pred_probs_multiannotator)\n",
    "    consensus_labels_tile = np.repeat(consensus_labels[:,np.newaxis], labels_multiannotator.shape[1], axis=1)\n",
    "    num_annotators_per_ex = np.count_nonzero(~np.isnan(labels), axis=1)\n",
    "    annotator_agreement = (labels_multiannotator == consensus_labels_tile) # Number of annotators matches consensus\n",
    "    annotator_agreement = annotator_agreement.sum(axis=1)\n",
    "    bin_consensus = (true_labels == consensus_labels) + 0\n",
    "    consensus_accuracy = pd.DataFrame(zip(annotator_agreement,bin_consensus), columns=['annotator_agreement','bin_consense'])\n",
    "    _ = consensus_accuracy.boxplot(by=['bin_consense'], figsize=(7,7))\n",
    "    consensus_accuracy = consensus_accuracy.groupby('bin_consense')[['annotator_agreement']].sum().reset_index()\n",
    "\n",
    "    return consensus_accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548fb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels_multiannotator(hlabels, htrue_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db88867",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_accuracy = get_consensus_accuracy_report(hlabels, \n",
    "                              htrue_labels, \n",
    "                              hannotator_mask, \n",
    "                              pred_probs_multiannotator)\n",
    "consensus_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ea0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_accuracy[['num_a','consensus_acc']].plot(kind='line',x='num_a',y='consensus_acc',color='pink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ad229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prec/recall plot for conensus_labels correct/not correct for each example given preds from model are now our scores\n",
    "# high level: when few annotators. unreliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebea05a",
   "metadata": {},
   "source": [
    "### Try random dropout of random number of examples\n",
    "- randomly flip some 1 bits in annotator mask per row\n",
    "- apply annotator mask to labels (nan where it is 0) and\n",
    "- labels_error_mask (false where it is 0)\n",
    "- calculate new pred_probs multiannotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels- all annotator labels np.full([height, width, 9], np.nan)\n",
    "# labels_error_mask- mask of annotator errors\n",
    "# annotator_mask- mask of what each person annotated\n",
    "\n",
    "def get_sample_labels(x_sample, y_sample, labels, labels_error_mask, annotator_mask):\n",
    "    s_annotator_mask = annotator_mask.copy()\n",
    "    s_annotator_mask[(x_sample,y_sample)] = 0\n",
    "    s_labels_error_mask = s_annotator_mask & labels_error_mask\n",
    "    s_labels = labels.copy()\n",
    "    np.copyto(s_labels, np.nan, where=s_annotator_mask)\n",
    "    print('Total idxs dropped: ', annotator_mask.sum() - s_annotator_mask.sum())\n",
    "    return s_labels, s_labels_error_mask, s_annotator_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329cb01a",
   "metadata": {},
   "source": [
    "**randomly drop x percent of all labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly drop x percent of all labels\n",
    "percent_dropped = 0.4\n",
    "\n",
    "x,y = np.where(hannotator_mask == 1)\n",
    "drop_idx = np.random.choice(np.arange(len(x)), int(len(x)*percent_dropped), replace=False)\n",
    "x_sample = x[drop_idx]\n",
    "y_sample = y[drop_idx]\n",
    "s_labels, s_labels_error_mask, s_annotator_mask = get_sample_labels(x_sample, y_sample, hlabels, hlabels_error_mask, hannotator_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_describe = plt_annotator_accuracy(s_labels_error_mask, s_annotator_mask)\n",
    "df_describe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411181a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_accuracy = get_consensus_accuracy_report(s_labels, \n",
    "                              htrue_labels, \n",
    "                              s_annotator_mask, \n",
    "                              pred_probs_multiannotator)\n",
    "consensus_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68954e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_accuracy[['num_a','consensus_acc']].plot(kind='line',x='num_a',y='consensus_acc',color='pink')\n",
    "# x axis can be lower bound on consensus labels (i.e. how many annotators agreed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78236f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels_multiannotator(s_labels, htrue_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60400057",
   "metadata": {},
   "source": [
    "**Drop x rows per every annotator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_dropped = 0.4\n",
    "rows_dropped = int(200 * percent_dropped)\n",
    "print('rows dropped: ', rows_dropped)\n",
    "\n",
    "x,y = np.where(hannotator_mask == 1)\n",
    "df_delete = pd.DataFrame(zip(x,y),columns=['x','y'])\n",
    "df_keep = df_delete.drop(df_delete.groupby('y').sample(n=200 - rows_dropped).index)\n",
    "x_sample,y_sample = df_keep['x'].values, df_keep['y'].values\n",
    "s_labels, s_labels_error_mask, s_annotator_mask = get_sample_labels(x_sample, y_sample, hlabels, hlabels_error_mask, hannotator_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_describe = plt_annotator_accuracy(s_labels_error_mask, s_annotator_mask)\n",
    "df_describe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_accuracy = get_consensus_accuracy_report(s_labels, \n",
    "                              htrue_labels, \n",
    "                              s_annotator_mask, \n",
    "                              pred_probs_multiannotator)\n",
    "consensus_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ec817",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_accuracy[['num_a','consensus_acc']].plot(kind='line',x='num_a',y='consensus_acc',color='pink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels_multiannotator(s_labels, htrue_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160cbe52",
   "metadata": {},
   "source": [
    "**Drop x values for every row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows have min 50 values\n",
    "percent_dropped = 0.4\n",
    "vals_dropped = int(50 * percent_dropped)\n",
    "print('vals dropped: ', vals_dropped)\n",
    "\n",
    "x,y = np.where(hannotator_mask == 1)\n",
    "df_delete = pd.DataFrame(zip(x,y),columns=['x','y'])\n",
    "df_keep = df_delete.drop(df_delete.groupby('x').sample(n=50 - vals_dropped).index)\n",
    "x_sample,y_sample = df_keep['x'].values, df_keep['y'].values\n",
    "s_labels, s_labels_error_mask, s_annotator_mask = get_sample_labels(x_sample, y_sample, hlabels, hlabels_error_mask, hannotator_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8711b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_accuracy = get_consensus_accuracy_report(s_labels, \n",
    "                              htrue_labels, \n",
    "                              s_annotator_mask, \n",
    "                              pred_probs_multiannotator)\n",
    "consensus_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels_multiannotator(s_labels, htrue_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e059999",
   "metadata": {},
   "source": [
    "### Load models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to map to display name\n",
    "method_adjust_pred_probs_display_dict = {\n",
    "    \"self_confidence-False\": \"Self Confidence\",\n",
    "    \"self_confidence-True\": \"Adjusted Self Confidence\",\n",
    "    \"normalized_margin-False\": \"Normalized Margin\",\n",
    "    \"normalized_margin-True\": \"Adjusted Normalized Margin\",\n",
    "    \"confidence_weighted_entropy-False\": \"Confidence Weighted Entropy\",\n",
    "    \"entropy-False\": \"Entropy\",\n",
    "    \"least_confidence-False\": \"Least Confidence\",\n",
    "}\n",
    "\n",
    "model_display_name_dict = {\"resnet18\": \"ResNet-18\",}\n",
    "\n",
    "models = [\"resnet18\"] # can also be: \"resnet50d\", \"efficientnet_b1\", \"twins_pcpvt_base\", \"swin_base_patch4_window7_224\"\n",
    "\n",
    "# args to pass to get_label_quality_scores()\n",
    "score_params = \\\n",
    "[\n",
    "    (\"self_confidence\", False),\n",
    "    (\"self_confidence\", True),\n",
    "    (\"normalized_margin\", False),\n",
    "    (\"normalized_margin\", True),\n",
    "    (\"confidence_weighted_entropy\", False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read numpy files from model_train_pred\n",
    "numpy_out_folder = './data/model_data_070622/'\n",
    "pred_probs = np.load(numpy_out_folder + \"test_pred_probs.npy\")\n",
    "pred_labels = np.load(numpy_out_folder + \"test_preds.npy\")\n",
    "true_labels = np.load(numpy_out_folder + \"test_labels.npy\")\n",
    "images = np.load(numpy_out_folder + \"test_images.npy\", allow_pickle=True)\n",
    "idxs = [int(image.split('/')[-1][-8:-4]) for image in images]\n",
    "\n",
    "# boolean mask of label errors\n",
    "labels = pred_labels # labels can change to annotator labels!!\n",
    "label_errors_target = labels != true_labels\n",
    "\n",
    "# set all cifar10h annotator data to the correct indexing\n",
    "htrue_labels = htrue_labels[idxs]\n",
    "htrue_images = htrue_images[idxs]\n",
    "hlabels = hlabels[idxs]\n",
    "hlabels_error_mask = hlabels_error_mask[idxs]\n",
    "hannotator_mask = hannotator_mask[idxs]\n",
    "\n",
    "assert np.array_equal(htrue_labels, true_labels) # check cifar10h sort matches what our model predicted on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9460e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "\n",
    "results = []\n",
    "results_list = []\n",
    "precision_recall_curves  = []\n",
    "for score_param in score_params:\n",
    "    method, adjust_pred_probs = score_param\n",
    "    # compute scores\n",
    "    label_quality_scores = get_label_quality_scores(labels=labels, pred_probs=pred_probs, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "    # compute accuracy of detecting label errors\n",
    "    auroc = roc_auc_score(label_errors_target, 1 - label_quality_scores)\n",
    "    # compute Lift@K evaluation metric\n",
    "    lift_at_k_dict = {}\n",
    "    for k in range(1000, 11000, 1000):\n",
    "        lift_at_k_dict[f\"lift_at_{k}\"] = lift_at_k(label_errors_target, 1 - label_quality_scores, k=k)\n",
    "    # save results\n",
    "    results = {\n",
    "        \"dataset\": \"cifar10\",\n",
    "        \"model\": \"resnet18\",\n",
    "        \"noise_config\": \"Noise Amount: 0.2 | Sparsity: 0.4\",\n",
    "        \"method\": method,\n",
    "        \"adjust_pred_probs\": adjust_pred_probs,\n",
    "        \"auroc\": auroc\n",
    "    }\n",
    "    # add the lift at k metrics\n",
    "    results.update(lift_at_k_dict)\n",
    "    # save results\n",
    "    results_list.append(results)\n",
    "    \n",
    "    # compute precision-recall curve using label quality scores\n",
    "    precision, recall, thresholds = precision_recall_curve(label_errors_target, 1 - label_quality_scores)\n",
    "    \n",
    "    # compute au-roc curve using label quality scores\n",
    "    fpr, tpr, thresholds = roc_curve(label_errors_target,  1 - label_quality_scores)\n",
    "    \n",
    "    precision_recall_curve_results = {\n",
    "        \"method\": method,\n",
    "        \"adjust_pred_probs\": adjust_pred_probs,\n",
    "        \"dataset\": \"cifar10\",\n",
    "        \"model\": \"resnet18\",\n",
    "        \"label_quality_scores\": label_quality_scores,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"thresholds\": thresholds\n",
    "    }\n",
    "\n",
    "#     # store precision-recall curve results\n",
    "#     precision_recall_curves.append(precision_recall_curve_results)\n",
    "    \n",
    "#     # store precision-recall curve results\n",
    "#     auroc_curves.append(precision_recall_curve_results)\n",
    "    \n",
    "    # plot prc\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(recall, precision, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "    plt.xlabel(\"Recall\", fontsize=14)\n",
    "    plt.ylabel(\"Precision\", fontsize=14)\n",
    "    plt.title(\"Precision-Recall Curve: Label Error Detection on CIFAR-10h \\n Model: resnet-18\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(fpr, tpr, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
    "    plt.title(\"AU ROC Curve: Label Error Detection on CIFAR-10h \\n Model: resnet-18\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# use better legend (i.e. self_confidence-True .. what is true?)\n",
    "# but overall no interest in showing different scores of single labels, show multiannotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f33c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe and export to csv\n",
    "\n",
    "df_result = pd.DataFrame(results_list)\n",
    "df_result.to_csv(f\"./data/benchmark_results/label_quality_scores_evaluation.csv\", index=False)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb69cd",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability threshold\n",
    "- Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n",
    "- Precision-Recall curves: imbalanced datasets (more sensitive to positive class), ROC curves: balanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd1f45",
   "metadata": {},
   "source": [
    "#### Analyze Consensus label vs model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc25b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much the consensus labels differ from\n",
    "hlabels_df = pd.DataFrame(hlabels)\n",
    "consensus_labels = get_consensus_labels(hlabels_df, pred_probs)\n",
    "\n",
    "print('Probability annotators alone correctly predict labels: ', (true_labels == consensus_labels).sum() / 10000)\n",
    "model_pred_labels = np.argmax(pred_probs, axis=1) # true labels == to what the model is likeley to predict\n",
    "print('Probability model alone correctly predicts labels: ', (model_pred_labels == true_labels).sum() / 10000) # suggests model is x% likeley to predict with the consensus\n",
    "print('Similar prediction between model preds and consensus_labels: ', (model_pred_labels == consensus_labels).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee9120",
   "metadata": {},
   "source": [
    "## Analyze label issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f95c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_issues(labels, pred_probs):\n",
    "    label_issues = find_label_issues(labels=labels,\n",
    "                        pred_probs=pred_probs,\n",
    "                        return_indices_ranked_by='self_confidence',\n",
    "                        )\n",
    "    issue_consensus_labels = labels[label_issues]\n",
    "    issue_images = images[label_issues]\n",
    "    issue_true_labels = true_labels[label_issues]\n",
    "    issue_is_issue = (issue_consensus_labels != issue_true_labels) + 0\n",
    "    issue_real_image_paths = [path + '/' + '/'.join(image.split('/')[-5:]) for image in issue_images]\n",
    "\n",
    "    print('Number of label issues detected: ', len(label_issues))\n",
    "    print('Number of true label issues: ', true_labels.shape[0] - np.sum(labels == true_labels))\n",
    "    print('Number of true label issues detected: ', np.sum(issue_is_issue))\n",
    "    \n",
    "    issues_df = pd.DataFrame(zip(issue_consensus_labels, issue_true_labels, issue_is_issue, issue_real_image_paths),\n",
    "            columns = ['label','true_label','is_issue','image_png'])\n",
    "    return issues_df\n",
    "\n",
    "def visualize_label_issues(issues_df, classes, scale=100):\n",
    "    print('Visualizing', len(issues_df), 'issues\\n')\n",
    "    \n",
    "    listOfImageNames = issues_df['image_png'].values\n",
    "\n",
    "    for index, row in issues_df.iterrows():\n",
    "        print('Correctly identified: ', bool(row['is_issue']), '\\nGiven label: ', classes[row['label']], '\\nTrue label: ', classes[row['true_label']],)\n",
    "        image = Image(filename=row['image_png'])\n",
    "        display(Image(filename=row['image_png'], width=scale))\n",
    "\n",
    "classes = {0:\"airplane\", \n",
    "           1:\"automobile\", \n",
    "           2:\"bird\", \n",
    "           3:\"cat\", \n",
    "           4:\"deer\",\n",
    "           5:\"dog\", \n",
    "           6:\"frog\", \n",
    "           7:\"horse\", \n",
    "           8:\"ship\", \n",
    "           9:\"truck\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd99baf",
   "metadata": {},
   "source": [
    "#### Run find_label_issues on the consensus labels and visualize label issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much the consensus labels differ from\n",
    "hlabels_df = pd.DataFrame(hlabels)\n",
    "consensus_labels = get_consensus_labels(hlabels_df, pred_probs)\n",
    "issues_df = get_label_issues(hlabels, pred_probs)\n",
    "issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_issues_df = issues_df[issues_df['is_issue'] == 1] # get a df of correctly identified true issues\n",
    "# visualize_label_issues(issues_df, classes)\n",
    "visualize_label_issues(true_issues_df, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da87ed",
   "metadata": {},
   "source": [
    "#### Run find_label_issues on an individual annotator's labels and visualize label issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_accuracy = hlabels_error_mask.sum(axis=0) / hannotator_mask.sum(axis=0)\n",
    "worst_annotator = np.argmin(annotator_accuracy)\n",
    "best_annotator = np.argmax(annotator_accuracy)\n",
    "print('worst annotator: ', worst_annotator, 'accuracy: ', annotator_accuracy[worst_annotator])\n",
    "print('best annotator: ', best_annotator, 'accuracy: ', annotator_accuracy[best_annotator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_id = 1 # worst = 2561, best = 1957\n",
    "\n",
    "a_hlabels = hlabels[:,annotator_id]\n",
    "a_hannotator_mask = hannotator_mask[:,annotator_id]\n",
    "a_hlabels_error_mask = hlabels_error_mask[:,annotator_id]\n",
    "\n",
    "a_labels = true_labels.copy()\n",
    "a_labels[a_hannotator_mask] = a_hlabels[a_hannotator_mask]\n",
    "\n",
    "print('Annotator accuracy: ', np.sum(a_labels[a_hannotator_mask] == true_labels[a_hannotator_mask]) / a_hannotator_mask.sum())\n",
    "print('Annotator accuracy: ', annotator_accuracy[annotator_id])\n",
    "print('Num correctly labeled points for annotator ', annotator_id, ': ', np.sum(a_labels == true_labels))\n",
    "print('Annotator + True label accuracy: ', np.sum(a_labels == true_labels) / len(a_labels))\n",
    "issues_df = get_label_issues(a_labels, pred_probs)\n",
    "issues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53230e66",
   "metadata": {},
   "source": [
    "## Using multiannotator library to analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d672ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hlabels_df = pd.DataFrame(hlabels)\n",
    "scores = get_label_quality_multiannotator(hlabels_df, pred_probs, return_annotator_stats=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b812f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = get_multiannotator_stats(hlabels_df, pred_probs, consensus_labels, 'agreement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697fed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_quality_scores = get_label_quality_scores(consensus_labels, pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(label_quality_scores, columns=['']).plot.hist(bins=30,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69069d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['worst_class'].plot.hist(bins=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats['overall_quality'].min(), stats['overall_quality'].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
