{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3aea21",
   "metadata": {},
   "source": [
    "# CL for Multiannotator Data Benchmarking [cifar10h]\n",
    "This notebook uses the results from the ``model_train_pred.ipynb`` notebook to evaluate model performance.\n",
    "\n",
    "Before running this notebook, make sure both cifar10 pngs and cifar10h datasets are downloaded locally:\n",
    "1. ``pip install cifar2png`` #install png installer for cifar10 images\n",
    "2. ``cifar2png cifar10 ./data/cifar10 --name-with-batch-index`` #download the cifar10 png images\n",
    "3. ``cd data & git clone https://github.com/jcpeterson/cifar-10h.git`` #download cifar-10h dataset\n",
    "5. Unizip ``cifar10h-raw`` and extract files out of ``cifar10-raw`` folder\n",
    "4. Run ``create_labels_df.ipynb`` to update image paths\n",
    "\n",
    "Notebook creates ``./data/model_data/cifar10_test_consensus_dataset.csv``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a969f20",
   "metadata": {},
   "source": [
    "# 1. Data Loading/Pre Process -- only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08c4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, precision_recall_curve, \\\n",
    "                            roc_curve, accuracy_score, log_loss, precision_recall_fscore_support\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from scipy import stats\n",
    "\n",
    "# cleanlab imports\n",
    "import cleanlab\n",
    "from cleanlab.multiannotator import get_label_quality_multiannotator, get_multiannotator_stats # only in hui wen directory\n",
    "from cleanlab.rank import get_label_quality_scores, get_label_quality_ensemble_scores\n",
    "from cleanlab.internal.label_quality_utils import get_normalized_entropy\n",
    "from cleanlab.filter import find_label_issues\n",
    "\n",
    "# local imports\n",
    "from utils.eval_metrics import lift_at_k\n",
    "from utils.active_learning_scores import least_confidence\n",
    "# experimental version of label quality ensemble scores with additional weighting schemes\n",
    "from utils.label_quality_ensemble_scores_experimental import get_label_quality_ensemble_scores_experimental\n",
    "\n",
    "path = os.getcwd()\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "fig_size_small = (5, 8)\n",
    "fig_size_big = (15,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69495f73",
   "metadata": {},
   "source": [
    "## Load Cifar10h Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b69b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this line throws an error, make sure you correctly downloaded and unzipped cifar10h-raw data\n",
    "pred_probs_multiannotator = np.load('./data/cifar10h/cifar10h-probs.npy')\n",
    "c10h_df = pd.read_csv('./data/cifar10h/cifar10h-raw.csv')\n",
    "c10h_df = c10h_df[c10h_df.cifar10_test_test_idx != -99999] # dropping all attention check trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e4d8b",
   "metadata": {},
   "source": [
    "#### Restructure dataset into following arrays:\n",
    "- ``c10h_num_datapoints: (N)``, ``c10h_num_annotators: (M)``\n",
    "- ``c10h_labels: (N,M)``\n",
    "- ``c10h_labels_error_mask: (N,M)`` where True is error\n",
    "- ``c10h_annotator_mask: (N,M)`` where True is annotator x anotated that\n",
    "- ``c10h_true_labels: (K=10,000,)``\n",
    "- ``c10h_true_images: (K=10,000,)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ad3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize arrays\n",
    "c10h_num_datapoints = c10h_df['cifar10_test_test_idx'].max() + 1\n",
    "c10h_num_annotators = c10h_df['annotator_id'].max() + 1\n",
    "\n",
    "c10h_labels = np.full((c10h_num_datapoints, c10h_num_annotators), np.nan) # all annotator labels np.full([height, width, 9], np.nan)\n",
    "c10h_labels_error_mask = np.zeros((c10h_num_datapoints, c10h_num_annotators), dtype=bool) # mask of annotator errors\n",
    "c10h_annotator_mask = np.zeros((c10h_num_datapoints, c10h_num_annotators), dtype=bool) # mask of what each person annotated\n",
    "\n",
    "c10h_true_labels = np.zeros((c10h_num_datapoints, ))\n",
    "c10h_true_images = np.empty((c10h_num_datapoints, ) ,dtype=object)\n",
    "\n",
    "c10h_annotator_data = (c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c869ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotator labels as numpy array (N=labels, M=annotators)\n",
    "for annotator_id in range(c10h_num_annotators):\n",
    "    adf = c10h_df[c10h_df.annotator_id == annotator_id] # 200 annotations per annotator\n",
    "    annotations_idx = adf['cifar10_test_test_idx'].values\n",
    "    annotations = adf['chosen_label'].values\n",
    "    errors = adf['correct_guess'].values\n",
    "    \n",
    "    c10h_labels[annotations_idx, annotator_id] = annotations\n",
    "    c10h_labels_error_mask[annotations_idx, annotator_id] = errors\n",
    "    c10h_annotator_mask[annotations_idx, annotator_id] = True\n",
    "\n",
    "# get true labels as numpy array (N = true labels,)\n",
    "idx_to_label = \\\n",
    "[(idx,label,image) for idx,label,image in zip(c10h_df['cifar10_test_test_idx'],c10h_df['true_label'],c10h_df['image_filename'])]\n",
    "idx_to_label = list(set(idx_to_label))\n",
    "\n",
    "idx = [idx_to_label[0] for idx_to_label in idx_to_label]\n",
    "true_label = [idx_to_label[1] for idx_to_label in idx_to_label]\n",
    "htrue_image = [idx_to_label[2] for idx_to_label in idx_to_label]\n",
    "\n",
    "c10h_true_labels[idx] = true_label\n",
    "c10h_true_images[idx] = htrue_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a8652",
   "metadata": {},
   "source": [
    "#### Helper functions for cifar10h dataset sampling and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d8c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of individual annotators on the points they labeled\n",
    "def plt_annotator_accuracy(labels_error_mask, annotator_mask):\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size_small\n",
    "    \n",
    "    annotator_accuracy = labels_error_mask.sum(axis=0) / annotator_mask.sum(axis=0)\n",
    "    plt.boxplot(annotator_accuracy, )\n",
    "    plt.show()\n",
    "\n",
    "    df_describe = pd.DataFrame(annotator_accuracy, columns=['score'])\n",
    "    return df_describe\n",
    "\n",
    "# Returns sample labels/error_mask/annotator_mask where x_drop, y_drop are idxs that are dropped\n",
    "def get_sample_labels(x_drop, y_drop, labels, labels_error_mask, annotator_mask):\n",
    "    s_annotator_mask = annotator_mask.copy()\n",
    "    s_annotator_mask[(x_drop,y_drop)] = 0\n",
    "    s_labels_error_mask = s_annotator_mask & labels_error_mask\n",
    "    s_labels = labels.copy()\n",
    "    np.copyto(s_labels, np.nan, where=(s_annotator_mask==0)) \n",
    "    print('Total idxs dropped: ', annotator_mask.sum() - s_annotator_mask.sum())\n",
    "    return s_labels, s_labels_error_mask, s_annotator_mask\n",
    "\n",
    "# Returns a list of labeled indices to drop (random percent_dropped% of all labels)\n",
    "def get_random_drop(c10h_annotator_mask, percent_dropped=0.4):\n",
    "    x,y = np.where( c10h_annotator_mask == 1)\n",
    "    drop_idx = np.random.choice(np.arange(len(x)), int(len(x)*percent_dropped), replace=False)\n",
    "    x_drop = x[drop_idx]\n",
    "    y_drop = y[drop_idx]\n",
    "    return x_drop, y_drop\n",
    "\n",
    "# Returns a list of labeled indices to drop (random length per row)\n",
    "def get_random_drop_per_row(c10h_annotator_mask):\n",
    "    x,y = np.where(c10h_annotator_mask == 1)\n",
    "    idx_df = pd.DataFrame(zip(x,y),columns=['x','y'])\n",
    "    for x_idx in range(idx_df['x'].max()+1):\n",
    "        num_drop = np.random.randint(1, len(idx_df[idx_df['x'] == x_idx])+1)\n",
    "        idx_df = idx_df.drop(idx_df[idx_df['x'] == x_idx].sample(num_drop).index)\n",
    "    x_drop = idx_df['x'].values\n",
    "    y_drop = idx_df['y'].values\n",
    "    return x_drop, y_drop\n",
    "\n",
    "# Returns a list of labeled indices to drop (random percent_dropped% of labels per annotator)\n",
    "def get_random_drop_per_annotator(c10h_annotator_mask, percent_dropped=0.4):\n",
    "    rows_dropped = int(200 * percent_dropped)\n",
    "    print('Total rows dropped per annotator: ', rows_dropped)\n",
    "    x,y = np.where( c10h_annotator_mask == 1)\n",
    "    df_delete = pd.DataFrame(zip(x,y),columns=['x','y'])\n",
    "    df_keep = df_delete.drop(df_delete.groupby('y').sample(n=200 - rows_dropped).index)\n",
    "    x_drop,y_drop = df_keep['x'].values, df_keep['y'].values\n",
    "    return x_drop, y_drop\n",
    "\n",
    "# Returns a list of labeled indices to drop \n",
    "# (Randomly drop until <= max_annotations per example. Try to minimize number of distinct annotators)\n",
    "def get_random_drop_per_row_min_annotators(c10h_annotator_mask, max_annotations = 5):\n",
    "    x,y = np.where(c10h_annotator_mask == 1)\n",
    "    xy = set([(x_idx,y_idx) for x_idx,y_idx in zip(x,y)])\n",
    "    idx_df = pd.DataFrame(zip(x,y),columns=['x','y'])\n",
    "    idx_keep = []\n",
    "    selected_annotators = set()\n",
    "    for x_idx in range(idx_df['x'].max()+1):\n",
    "        Y = idx_df[idx_df['x'] == x_idx]['y']\n",
    "        seen_annotators = set(Y).intersection(selected_annotators)\n",
    "        if len(seen_annotators) < max_annotations: # We need to randomly select more annotators to greedy add\n",
    "            num_to_find = max_annotations - len(seen_annotators)\n",
    "            y_keep = set(np.random.choice(list(set(Y).difference(seen_annotators)), num_to_find, replace=False))\n",
    "            selected_annotators = selected_annotators.union(y_keep)\n",
    "            y_keep = seen_annotators.union(y_keep)\n",
    "        else: # We have enough annotators and need to randomly select annotations out of the guys we have\n",
    "            y_keep = np.random.choice(list(seen_annotators), max_annotations,replace=False)\n",
    "        xy_keep = [(x_idx,y) for y in y_keep]\n",
    "        idx_keep.extend(xy_keep)\n",
    "    xy = xy.difference(set(idx_keep))\n",
    "    x_drop = [xy_idx[0] for xy_idx in xy]\n",
    "    y_drop = [xy_idx[1] for xy_idx in xy]\n",
    "    return x_drop, y_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4f11f",
   "metadata": {},
   "source": [
    "#### Helper functions for getting and analyzing consensus labels\n",
    "- consensus_labels are determined based on the individual labels of all annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfa1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines the consensus_labels\n",
    "def get_consensus_labels(labels_multiannotator, pred_probs=None):\n",
    "    \n",
    "    labels = np.array(labels_multiannotator) # needs to be numpy\n",
    "    if pred_probs is None:\n",
    "        pred_counts = labels.sum(axis=1)\n",
    "        pred_probs = labels / pred_counts[:,np.newaxis]\n",
    "        \n",
    "    mode_labels_multiannotator = labels_multiannotator.mode(axis=1)\n",
    "    consensus_labels = []\n",
    "    for i in range(len(mode_labels_multiannotator)):\n",
    "        consensus_labels.append( int(mode_labels_multiannotator.iloc[i][pred_probs[i][mode_labels_multiannotator.iloc[i].dropna().astype(int).to_numpy()\n",
    "                    ].argmax()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    return np.array(consensus_labels)\n",
    "\n",
    "# Get consensus labels and calculate accuracy, precision, recall, f1\n",
    "def get_consensus_label_accuracy(labels, pred_probs, true_labels):\n",
    "    labels_df = pd.DataFrame(labels)\n",
    "    consensus_labels = get_consensus_labels(labels_df, pred_probs)\n",
    "    consensus_labels_accuracy = (true_labels == consensus_labels).sum() / 10000\n",
    "    print('Consensus label accuracy: ', consensus_labels_accuracy)\n",
    "    print('\\nPer class scores:')\n",
    "    p, r, f1, _ = precision_recall_fscore_support(true_labels, consensus_labels)\n",
    "    results_df = pd.DataFrame(zip(p,r,f1), columns=['precision', 'recall', 'f1'])\n",
    "    return results_df\n",
    "\n",
    "# Plots the distribution of annotator agreement for correct/incorrect labels\n",
    "def plot_labels_multiannotator(labels, true_labels, pred_probs_multiannotator=None):\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size_small\n",
    "    \n",
    "    labels_multiannotator = pd.DataFrame(labels)\n",
    "    \n",
    "    if pred_probs_multiannotator is None:\n",
    "        pred_counts = labels.sum(axis=1)\n",
    "        pred_probs_multiannotator = labels / pred_counts[:,np.newaxis]\n",
    "\n",
    "    consensus_labels = get_consensus_labels(labels_multiannotator, pred_probs_multiannotator)\n",
    "    consensus_labels_tile = np.repeat(consensus_labels[:,np.newaxis], labels_multiannotator.shape[1], axis=1)\n",
    "    c10h_num_annotators_per_ex = np.count_nonzero(~np.isnan(labels), axis=1)\n",
    "    annotator_agreement = (labels_multiannotator == consensus_labels_tile) # Number of annotators matches consensus\n",
    "    annotator_agreement = annotator_agreement.sum(axis=1)\n",
    "    bin_consensus = (true_labels == consensus_labels) + 0\n",
    "    consensus_accuracy = pd.DataFrame(zip(annotator_agreement,bin_consensus), columns=['annotator_agreement','bin_consense'])\n",
    "    _ = consensus_accuracy.boxplot(by=['bin_consense'], figsize=(7,7))\n",
    "    consensus_accuracy = consensus_accuracy.groupby('bin_consense')[['annotator_agreement']].sum().reset_index()\n",
    "\n",
    "    return consensus_accuracy\n",
    "\n",
    "# Returns spearman correlation given two scores, checks for nans\n",
    "def get_spearman_correlation(x, y):\n",
    "    num_nans_x = np.sum(np.isnan(x))\n",
    "    num_nans_y = np.sum(np.isnan(y))\n",
    "    \n",
    "    if num_nans_x > 0:\n",
    "        x = np.nan_to_num(x)\n",
    "        print('First param contains nans. Replacing', num_nans_x, 'nans with 0')\n",
    "    if num_nans_y > 0:\n",
    "        y = np.nan_to_num(y)\n",
    "        print('First param contains nans. Replacing', num_nans_y, 'nans with 0')\n",
    "\n",
    "    return stats.spearmanr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e059999",
   "metadata": {},
   "source": [
    "## Load Cifar10 test and model output data\n",
    "- model can be trained/predictions can be generated with the ``model_train_pred.ipynb`` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf25dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to map to display name\n",
    "method_adjust_pred_probs_display_dict = {\n",
    "    \"self_confidence-False\": \"Self Confidence\",\n",
    "    \"self_confidence-True\": \"Adjusted Self Confidence\",\n",
    "    \"normalized_margin-False\": \"Normalized Margin\",\n",
    "    \"normalized_margin-True\": \"Adjusted Normalized Margin\",\n",
    "    \"confidence_weighted_entropy-False\": \"Confidence Weighted Entropy\",\n",
    "    \"entropy-False\": \"Entropy\",\n",
    "    \"least_confidence-False\": \"Least Confidence\",\n",
    "}\n",
    "\n",
    "model_display_name_dict = {\"resnet18\": \"ResNet-18\",}\n",
    "\n",
    "# args to pass to get_label_quality_scores()\n",
    "score_params = \\\n",
    "[\n",
    "    (\"self_confidence\", False),\n",
    "    (\"self_confidence\", True),\n",
    "    (\"normalized_margin\", False),\n",
    "    (\"normalized_margin\", True),\n",
    "    (\"confidence_weighted_entropy\", False)\n",
    "]\n",
    "\n",
    "# args to pass to get_label_quality_scores()\n",
    "# TODO: add more consensus metrics\n",
    "score_params_consensus = \\\n",
    "[\n",
    "    (\"majority\", False),\n",
    "#     (\"dawid-skeen\", False),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb5a26",
   "metadata": {},
   "source": [
    "#### Helper functions to match label indices, calculate and plot model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d9460e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches indices of model trained data to cifar10h annotation dataset\n",
    "def match_label_indices_old(numpy_out_folder, c10h_annotator_data, model):\n",
    "    # read numpy files from model_train_pred\n",
    "    pred_probs = np.load(numpy_out_folder + \"test_pred_probs_\" + model + \".npy\")\n",
    "    pred_labels = np.load(numpy_out_folder + \"test_preds_\" + model + \".npy\")\n",
    "    true_labels = np.load(numpy_out_folder + \"test_labels_\" + model + \".npy\")\n",
    "    images = np.load(numpy_out_folder + \"test_images_\" + model + \".npy\", allow_pickle=True)\n",
    "    idxs = [int(image.split('/')[-1][-8:-4]) for image in images]\n",
    "    \n",
    "    # set all cifar10h annotator data to the correct indexing\n",
    "    c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images = c10h_annotator_data\n",
    "    \n",
    "    c10h_true_labels = c10h_true_labels[idxs]\n",
    "    c10h_true_images = c10h_true_images[idxs]\n",
    "    c10h_labels = c10h_labels[idxs]\n",
    "    c10h_labels_error_mask = c10h_labels_error_mask[idxs]\n",
    "    c10h_annotator_mask =  c10h_annotator_mask[idxs]\n",
    "    \n",
    "    # check cifar10h ordering matches what our model predicted on\n",
    "    assert np.array_equal(c10h_true_labels, true_labels)\n",
    " \n",
    "    c10h_annotator_data = (c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images)\n",
    "    c10_model_data = (pred_probs, pred_labels, true_labels, images)\n",
    "    \n",
    "    return c10_model_data, c10h_annotator_data\n",
    "\n",
    "# Matches indices of model trained data to cifar10h annotation dataset\n",
    "def match_label_indices(numpy_out_folder, c10h_annotator_data):\n",
    "    # read numpy files from model_train_pred\n",
    "    pred_probs = np.load(numpy_out_folder + \"pred_probs.npy\")\n",
    "    pred_labels = np.load(numpy_out_folder + \"true_labels.npy\")\n",
    "    true_labels = np.load(numpy_out_folder + \"labels.npy\")\n",
    "    images = np.load(numpy_out_folder + \"images.npy\", allow_pickle=True)\n",
    "    idxs = [int(image.split('/')[-1][-8:-4]) for image in images]\n",
    "    \n",
    "    print(idxs[:10])\n",
    "    print(images[:10])\n",
    "    # set all cifar10h annotator data to the correct indexing\n",
    "    c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images = c10h_annotator_data\n",
    "    \n",
    "    print(c10h_true_images[:10])\n",
    "    c10h_true_labels = c10h_true_labels[idxs]\n",
    "    c10h_true_images = c10h_true_images[idxs]\n",
    "    c10h_labels = c10h_labels[idxs]\n",
    "    c10h_labels_error_mask = c10h_labels_error_mask[idxs]\n",
    "    c10h_annotator_mask =  c10h_annotator_mask[idxs]\n",
    "    \n",
    "    # check cifar10h ordering matches what our model predicted on\n",
    "    assert np.array_equal(c10h_true_labels, true_labels)\n",
    " \n",
    "    c10h_annotator_data = (c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images)\n",
    "    c10_model_data = (pred_probs, pred_labels, true_labels, images)\n",
    "    \n",
    "    return c10_model_data, c10h_annotator_data\n",
    "\n",
    "\n",
    "#c10h true labels passed for assertion (checks idx just in case)\n",
    "def benchmark_results_base(labels, true_labels, score_params, model):\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size_big\n",
    "        \n",
    "    # create boolean mask of label errors\n",
    "    label_errors_target = labels != true_labels\n",
    "    \n",
    "    results = []\n",
    "    results_list = []\n",
    "\n",
    "    for score_param in score_params:\n",
    "        method, adjust_pred_probs = score_param\n",
    "        # compute scores\n",
    "        label_quality_scores = get_label_quality_scores(labels=labels, pred_probs=pred_probs, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "        # compute accuracy of detecting label errors\n",
    "        auroc = roc_auc_score(label_errors_target, 1 - label_quality_scores)\n",
    "        # compute Lift@K evaluation metric\n",
    "        lift_at_k_dict = {}\n",
    "        for k in range(1000, 11000, 1000):\n",
    "            lift_at_k_dict[f\"lift_at_{k}\"] = lift_at_k(label_errors_target, 1 - label_quality_scores, k=k)\n",
    "        # save results\n",
    "        results = {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"model\": model,\n",
    "            \"noise_config\": \"nan\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"auroc\": auroc\n",
    "        }\n",
    "        # add the lift at k metrics\n",
    "        results.update(lift_at_k_dict)\n",
    "        # save results\n",
    "        results_list.append(results)\n",
    "\n",
    "        # compute precision-recall curve using label quality scores\n",
    "        precision, recall, thresholds = precision_recall_curve(label_errors_target, 1 - label_quality_scores)\n",
    "\n",
    "        # compute au-roc curve using label quality scores\n",
    "        fpr, tpr, thresholds = roc_curve(label_errors_target,  1 - label_quality_scores)\n",
    "\n",
    "        precision_recall_curve_results = {\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"model\": model,\n",
    "            \"label_quality_scores\": label_quality_scores,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"thresholds\": thresholds\n",
    "        }\n",
    "        \n",
    "        # plot prc\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(recall, precision, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "        plt.xlabel(\"Recall\", fontsize=14)\n",
    "        plt.ylabel(\"Precision\", fontsize=14)\n",
    "        plt.title(\"Precision-Recall Curve: Label Error Detection on CIFAR-10 \\n Model: resnet-18\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(fpr, tpr, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "        plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
    "        plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
    "        plt.title(\"AU ROC Curve: Label Error Detection on CIFAR-10 \\n Model: resnet-18\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.legend()\n",
    "        \n",
    "    return results_list, precision_recall_curve_results\n",
    "\n",
    "# TODO: add back in adjusted_pred_probs once multiannotator has that param\n",
    "def benchmark_results_annotators(c10h_labels, c10h_true_labels, score_params, model):\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size_big    \n",
    "    results = []\n",
    "    results_list = []\n",
    "    \n",
    "    c10h_labels = pd.DataFrame(c10h_labels)\n",
    "\n",
    "    for score_param in score_params:\n",
    "        method, adjust_pred_probs = score_param\n",
    "\n",
    "        label_quality_multiannotator, _ = get_label_quality_multiannotator(c10h_labels, pred_probs, consensus_method=method, return_annotator_stats = True, verbose=False)\n",
    "        # create boolean mask of label errors\n",
    "        labels = label_quality_multiannotator['consensus_label'] \n",
    "        label_errors_target = labels != c10h_true_labels # labels can change to annotator labels!!\n",
    "        # compute scores\n",
    "        label_quality_scores = label_quality_multiannotator['quality_of_consensus']\n",
    "        # compute accuracy of detecting label errors\n",
    "        auroc = roc_auc_score(label_errors_target, 1 - label_quality_scores)\n",
    "        # compute Lift@K evaluation metric\n",
    "        lift_at_k_dict = {}\n",
    "        for k in range(1000, 11000, 1000):\n",
    "            lift_at_k_dict[f\"lift_at_{k}\"] = lift_at_k(label_errors_target, 1 - label_quality_scores, k=k)\n",
    "        # save results\n",
    "        results = {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"model\": model,\n",
    "            \"noise_config\": \"nan\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"auroc\": auroc\n",
    "        }\n",
    "        # add the lift at k metrics\n",
    "        results.update(lift_at_k_dict)\n",
    "        # save results\n",
    "        results_list.append(results)\n",
    "\n",
    "        # compute precision-recall curve using label quality scores\n",
    "        precision, recall, thresholds = precision_recall_curve(label_errors_target, 1 - label_quality_scores)\n",
    "\n",
    "        # compute au-roc curve using label quality scores\n",
    "        fpr, tpr, thresholds = roc_curve(label_errors_target,  1 - label_quality_scores)\n",
    "\n",
    "        precision_recall_curve_results = {\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"model\": model,\n",
    "            \"label_quality_scores\": label_quality_scores,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"thresholds\": thresholds\n",
    "        }\n",
    "        \n",
    "        # plot prc\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(recall, precision, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "        plt.xlabel(\"Recall\", fontsize=14)\n",
    "        plt.ylabel(\"Precision\", fontsize=14)\n",
    "        plt.title(\"Precision-Recall Curve \\n Model: resnet-18\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(fpr, tpr, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "        plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
    "        plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
    "        plt.title(\"AU ROC Curve \\n Model: resnet-18\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.legend()\n",
    "        \n",
    "    return results_list, precision_recall_curve_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830368a0",
   "metadata": {},
   "source": [
    "### Load cifar10test and model output data [this does indexing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2709d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get data\n",
    "# model_folder = 'model_data_070622'\n",
    "# numpy_out_folder = './data/' + model_folder + '/' # folder containing model train data\n",
    "# model = \"resnet18\" # can also be: \"swin_base_patch4_window7_224\"\n",
    "# c10_model_data, c10h_annotator_data = match_label_indices_old(numpy_out_folder, c10h_annotator_data, model)\n",
    "\n",
    "# # unpack fixed indices\n",
    "# c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images = c10h_annotator_data\n",
    "# pred_probs, pred_labels, true_labels, images = c10_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ec7fe",
   "metadata": {},
   "source": [
    "### Optional: Drop out annotators from data\n",
    "- Use either ``get_random_drop_per_annotator()``, ``get_random_drop_per_row_min_annotators()``, ``get_random_drop_per_row()``, ``get_random_drop()``\n",
    "- TODO: ``get_random_drop_per_row()`` very slow, needs to be optimized with lambda\n",
    "- TODO: Fix loopy logic in ``get_random_drop_per_row_min_annotators()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73375f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total idxs dropped:  464200\n",
      "Make sure 5 <= max_annotations and 5 > 0: \n",
      "Total idxs dropped:  20071\n",
      "Make sure 5 <= max_annotators and 1 > 0: \n",
      "(10000, 421) (10000, 421)\n"
     ]
    }
   ],
   "source": [
    "x_drop, y_drop = get_random_drop_per_row_min_annotators(c10h_annotator_mask, 5)\n",
    "c10h_labels, c10h_labels_error_mask, c10h_annotator_mask = \\\n",
    "                    get_sample_labels(x_drop, y_drop, c10h_labels, c10h_labels_error_mask, c10h_annotator_mask)\n",
    "\n",
    "print(f'Make sure {c10h_annotator_mask.sum(axis=1).max()} <= max_annotations and { c10h_annotator_mask.sum(axis=1).min()} > 0: ')\n",
    "\n",
    "x_drop, y_drop = get_random_drop_per_row(c10h_annotator_mask)\n",
    "c10h_labels, c10h_labels_error_mask, c10h_annotator_mask = \\\n",
    "                    get_sample_labels(x_drop, y_drop, c10h_labels, c10h_labels_error_mask, c10h_annotator_mask)\n",
    "print(f'Make sure {c10h_annotator_mask.sum(axis=1).max()} <= max_annotators and { c10h_annotator_mask.sum(axis=1).min()} > 0: ')\n",
    "\n",
    "# drop all empty annotators\n",
    "drop_axis = c10h_labels.copy()\n",
    "c10h_labels = c10h_labels[:, ~np.isnan(drop_axis).all(axis=0)]\n",
    "c10h_labels_error_mask = c10h_labels_error_mask[:, ~np.isnan(drop_axis).all(axis=0)]\n",
    "c10h_annotator_mask = c10h_annotator_mask[:, ~np.isnan(drop_axis).all(axis=0)]\n",
    "\n",
    "print(c10h_labels.shape, c10h_annotator_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d2a339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'0'}>]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(c10h_annotator_mask.sum(axis=0)).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449dc813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(c10h_annotator_mask.sum(axis=0) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "220aa755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'0'}>]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(c10h_annotator_mask.sum(axis=1)).hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cf4e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./benchmark_data/c10h_labels_range_1_5', c10h_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154cf724",
   "metadata": {},
   "source": [
    "### Get consensus labels & save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a7aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.multiannotator import _get_consensus_label\n",
    "consensus_labels = _get_consensus_label(pd.DataFrame(c10h_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa70bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: lambda create new column\n",
    "mini_df = c10h_df[['image_filename','true_category','cifar10_test_test_idx']].drop_duplicates()\n",
    "image_locs = [path + \n",
    "              '/data/cifar10/test/' +\n",
    "              mini_df[mini_df['image_filename'] == c10h_true_images[i]]['true_category'].item() +\n",
    "              '/test_batch_index_' +\n",
    "              str(mini_df[mini_df['image_filename'] == c10h_true_images[i]]['cifar10_test_test_idx'].item()).zfill(4) +\n",
    "              '.png'\n",
    "              for i in range(len(c10h_true_images))]\n",
    "consensus_df = pd.DataFrame(zip(image_locs, consensus_labels), columns=['image', 'label'])\n",
    "consensus_df.to_csv('./benchmark_data/cifar10_test_consensus_dataset_range_1_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df7cf6d",
   "metadata": {},
   "source": [
    "# 2. Benchmark Evaluation -- experiment as needed\n",
    "\n",
    "**Major Benchmarks**\n",
    "- How good is consensus quality-score for any given consensus\n",
    "    - Plots: auroc, auprc\n",
    "    - Scores: auroc\n",
    "- How accurate are consensus labels: accuracy\n",
    "    - Overall: accuracy\n",
    "    - Per-class: precision,recall,f1 scores\n",
    "- How good are annotator-quality scores\n",
    "    - Distribution of individual annotator accuracy vs ground truth\n",
    "    - Spearman correlation between annotator scores vs annotator accuracy\n",
    "\n",
    "**Side Benchmarks**\n",
    "- Annotator Agreement Consensus vs. Label Accuracy\n",
    "- Annotator vs Model Accuracy\n",
    "- Label quality scores distribution\n",
    "- Worst Class per Annotator distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3fb618",
   "metadata": {},
   "source": [
    "## How good is consensus quality-score for any given consensus \n",
    "## [model baseline]\n",
    "#### AUROC vs AUPRC curves\n",
    "- ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability threshold\n",
    "- Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n",
    "- Precision-Recall curves: imbalanced datasets (more sensitive to positive class), ROC curves: balanced datasets\n",
    "- Data behavior in curves is relatively similar (i.e. well performing label will perform well in both curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702132b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_base, precision_recall_curve_results_base = benchmark_results_base(pred_labels, true_labels, score_params, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fb2fa",
   "metadata": {},
   "source": [
    "## [consensus labels, same metrics as base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_base_con, precision_recall_curve_results_base_con = benchmark_results_base(consensus_labels, c10h_true_labels, score_params, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce59275",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list_base_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66244f7",
   "metadata": {},
   "source": [
    "## [consensus labels, new label_quality scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow because calculates label_quality_multiannotator for each annotator ~ 1 min each\n",
    "results_list_consensus, precision_recall_curve_results_consensus = benchmark_results_annotators(c10h_labels, c10h_true_labels, score_params_consensus, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73667ee0",
   "metadata": {},
   "source": [
    "### AUROC score per method\n",
    "**Base model with labels = model predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ccc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651b470",
   "metadata": {},
   "source": [
    "**Base score calculations with labels = consensus labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list_base_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3874ad",
   "metadata": {},
   "source": [
    "**Different Multiannotator label quality scoring methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72902b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list_consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14a73e",
   "metadata": {},
   "source": [
    "## How accurate are consensus labels\n",
    "#### Overall accuracy and per-class precision, recall, f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_consensus_label_accuracy(c10h_labels, pred_probs, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffe4cf",
   "metadata": {},
   "source": [
    "## How good are annotator-quality scores \n",
    "#### Distribution of individual annotator accuracy vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c616c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_accuracy_df = plt_annotator_accuracy(c10h_labels_error_mask,  c10h_annotator_mask)\n",
    "annotator_accuracy_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292acf9",
   "metadata": {},
   "source": [
    "#### Spearman Correlation\n",
    "- Nonparametric measure of the monotonicity of the relationship between two datasets\n",
    "- Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.\n",
    "- The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. The p-values are not entirely reliable\n",
    "- TODO: figure out why spearman correlation is 0 when we randomly remove random number of datapoints per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_quality_multiannotator, multiannotator_stats = get_label_quality_multiannotator(pd.DataFrame(c10h_labels), pred_probs, return_annotator_stats = True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_score = multiannotator_stats['overall_quality'].values\n",
    "accuracy = annotator_accuracy_df['score'].values\n",
    "get_spearman_correlation(quality_score, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3860339",
   "metadata": {},
   "source": [
    "## Side Benchmarks\n",
    "#### Annotator Agreement Consensus vs. Label Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels_multiannotator(c10h_labels, c10h_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd1f45",
   "metadata": {},
   "source": [
    "#### Annotator vs Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc25b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much the consensus labels differ from\n",
    "c10h_labels_df = pd.DataFrame(c10h_labels)\n",
    "consensus_labels = get_consensus_labels(c10h_labels_df, pred_probs)\n",
    "\n",
    "print('Probability annotators alone correctly predict labels: ', (true_labels == consensus_labels).sum() / 10000)\n",
    "model_pred_labels = np.argmax(pred_probs, axis=1) # true labels == to what the model is likeley to predict\n",
    "print('Probability model alone correctly predicts labels: ', (model_pred_labels == true_labels).sum() / 10000) # suggests model is x% likeley to predict with the consensus\n",
    "print('Similar prediction between model preds and consensus_labels: ', (model_pred_labels == consensus_labels).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deba0e3",
   "metadata": {},
   "source": [
    "#### Label quality scores distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39726d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_quality_scores = get_label_quality_scores(consensus_labels, pred_probs)\n",
    "pd.DataFrame(label_quality_scores, columns=['']).plot.hist(bins=30,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aaffb4",
   "metadata": {},
   "source": [
    "#### Worst Class Per Annotator Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9082ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiannotator_stats = get_multiannotator_stats(c10h_labels_df, pred_probs, consensus_labels, 'agreement')\n",
    "multiannotator_stats['worst_class'].plot.hist(bins=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee9120",
   "metadata": {},
   "source": [
    "# 3. Label Issue Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f95c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0:\"airplane\", \n",
    "           1:\"automobile\", \n",
    "           2:\"bird\", \n",
    "           3:\"cat\", \n",
    "           4:\"deer\",\n",
    "           5:\"dog\", \n",
    "           6:\"frog\", \n",
    "           7:\"horse\", \n",
    "           8:\"ship\", \n",
    "           9:\"truck\"}\n",
    "\n",
    "def get_label_issues(labels, pred_probs, true_labels, images):\n",
    "    label_issues = find_label_issues(labels=labels,\n",
    "                        pred_probs=pred_probs,\n",
    "                        return_indices_ranked_by='self_confidence',\n",
    "                        )\n",
    "    issue_consensus_labels = labels[label_issues]\n",
    "    issue_images = images[label_issues]\n",
    "    issue_true_labels = true_labels[label_issues]\n",
    "    issue_is_issue = (issue_consensus_labels != issue_true_labels) + 0\n",
    "    issue_real_image_paths = [path + '/' + '/'.join(image.split('/')[-5:]) for image in issue_images]\n",
    "\n",
    "    print('Number of label issues detected: ', len(label_issues))\n",
    "    print('Number of true label issues: ', true_labels.shape[0] - np.sum(labels == true_labels))\n",
    "    print('Number of true label issues detected: ', np.sum(issue_is_issue))\n",
    "    \n",
    "    issues_df = pd.DataFrame(zip(issue_consensus_labels, issue_true_labels, issue_is_issue, issue_real_image_paths),\n",
    "            columns = ['label','true_label','is_issue','image_png'])\n",
    "    return issues_df\n",
    "\n",
    "def visualize_label_issues(issues_df, classes, scale=100):\n",
    "    print('Visualizing', len(issues_df), 'issues\\n')\n",
    "    \n",
    "    listOfImageNames = issues_df['image_png'].values\n",
    "\n",
    "    for index, row in issues_df.iterrows():\n",
    "        print('Correctly identified: ', bool(row['is_issue']), '\\nGiven label: ', classes[row['label']], '\\nTrue label: ', classes[row['true_label']],)\n",
    "        image = Image(filename=row['image_png'])\n",
    "        display(Image(filename=row['image_png'], width=scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd99baf",
   "metadata": {},
   "source": [
    "#### Run find_label_issues on the consensus labels and visualize label issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much the consensus label issues differ from true labels\n",
    "c10h_labels_df = pd.DataFrame(c10h_labels)\n",
    "consensus_labels = get_consensus_labels(c10h_labels_df, pred_probs)\n",
    "issues_df = get_label_issues(consensus_labels, pred_probs, true_labels, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_issues_df = issues_df[issues_df['is_issue'] == 1] # get a df of correctly identified true issues\n",
    "# visualize_label_issues(issues_df, classes)\n",
    "visualize_label_issues(true_issues_df, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da87ed",
   "metadata": {},
   "source": [
    "#### Run find_label_issues on an individual annotator's labels and visualize label issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_accuracy = c10h_labels_error_mask.sum(axis=0) /  c10h_annotator_mask.sum(axis=0)\n",
    "worst_annotator = np.argmin(annotator_accuracy)\n",
    "best_annotator = np.argmax(annotator_accuracy)\n",
    "print('worst annotator: ', worst_annotator, 'accuracy: ', annotator_accuracy[worst_annotator])\n",
    "print('best annotator: ', best_annotator, 'accuracy: ', annotator_accuracy[best_annotator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_id = 1 # worst = 2561, best = 1957\n",
    "\n",
    "a_c10h_labels = c10h_labels[:,annotator_id]\n",
    "a_c10h_annotator_mask =  c10h_annotator_mask[:,annotator_id]\n",
    "a_c10h_labels_error_mask = c10h_labels_error_mask[:,annotator_id]\n",
    "\n",
    "a_labels = true_labels.copy()\n",
    "a_labels[a_c10h_annotator_mask] = a_c10h_labels[a_c10h_annotator_mask]\n",
    "\n",
    "print('Annotator accuracy: ', np.sum(a_labels[a_c10h_annotator_mask] == true_labels[a_c10h_annotator_mask]) / a_c10h_annotator_mask.sum())\n",
    "print('Annotator accuracy: ', annotator_accuracy[annotator_id])\n",
    "print('Num correctly labeled points for annotator ', annotator_id, ': ', np.sum(a_labels == true_labels))\n",
    "print('Annotator + True label accuracy: ', np.sum(a_labels == true_labels) / len(a_labels))\n",
    "\n",
    "issues_df = get_label_issues(a_labels, pred_probs, true_labels, images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
