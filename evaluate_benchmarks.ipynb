{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3aea21",
   "metadata": {},
   "source": [
    "# CL for Multiannotator Data Benchmarking [cifar10h]\n",
    "This notebook uses the results from the ``model_train_pred.ipynb`` notebook to evaluate model performance.\n",
    "\n",
    "Before running this notebook, make sure both cifar10 pngs and cifar10h datasets are downloaded locally:\n",
    "1. ``pip install cifar2png`` #install png installer for cifar10 images\n",
    "2. ``cifar2png cifar10 ./data/cifar10 --name-with-batch-index`` #download the cifar10 png images\n",
    "3. ``cd data & git clone https://github.com/jcpeterson/cifar-10h.git`` #download cifar-10h dataset\n",
    "5. Unizip ``cifar10h-raw`` and extract files out of ``cifar10-raw`` folder\n",
    "4. Run ``create_labels_df.ipynb`` to update image paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a969f20",
   "metadata": {},
   "source": [
    "# 1. Data Loading/Pre Process -- only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, precision_recall_curve, \\\n",
    "                            roc_curve, accuracy_score, log_loss, precision_recall_fscore_support\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from scipy import stats\n",
    "\n",
    "# cleanlab imports\n",
    "import cleanlab\n",
    "from cleanlab.multiannotator import get_label_quality_multiannotator, get_multiannotator_stats # only in hui wen directory\n",
    "from cleanlab.rank import get_label_quality_scores, get_label_quality_ensemble_scores\n",
    "from cleanlab.internal.label_quality_utils import get_normalized_entropy\n",
    "from cleanlab.filter import find_label_issues\n",
    "\n",
    "# local imports\n",
    "from utils.eval_metrics import lift_at_k\n",
    "from utils.active_learning_scores import least_confidence\n",
    "# experimental version of label quality ensemble scores with additional weighting schemes\n",
    "from utils.label_quality_ensemble_scores_experimental import get_label_quality_ensemble_scores_experimental\n",
    "\n",
    "path = os.getcwd()\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "fig_size_small = (5, 8)\n",
    "fig_size_big = (15,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69495f73",
   "metadata": {},
   "source": [
    "## Load Cifar10h Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b69b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this line throws an error, make sure you correctly downloaded and unzipped cifar10h-raw data\n",
    "pred_probs_multiannotator = np.load('./data/cifar10h/cifar10h-probs.npy')\n",
    "c10h_df = pd.read_csv('./data/cifar10h/cifar10h-raw.csv')\n",
    "c10h_df = c10h_df[c10h_df.cifar10_test_test_idx != -99999] # dropping all attention check trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e4d8b",
   "metadata": {},
   "source": [
    "#### Restructure dataset into following arrays:\n",
    "- ``c10h_num_datapoints: (N)``, ``c10h_num_annotators: (M)``\n",
    "- ``c10h_labels: (N,M)``\n",
    "- ``c10h_labels_error_mask: (N,M)`` where True is error\n",
    "- ``c10h_annotator_mask: (N,M)`` where True is annotator x anotated that\n",
    "- ``c10h_true_labels: (K=10,000,)``\n",
    "- ``c10h_true_images: (K=10,000,)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize arrays\n",
    "c10h_num_datapoints = c10h_df['cifar10_test_test_idx'].max() + 1\n",
    "c10h_num_annotators = c10h_df['annotator_id'].max() + 1\n",
    "\n",
    "c10h_labels = np.full((c10h_num_datapoints, c10h_num_annotators), np.nan) # all annotator labels np.full([height, width, 9], np.nan)\n",
    "c10h_labels_error_mask = np.zeros((c10h_num_datapoints, c10h_num_annotators), dtype=bool) # mask of annotator errors\n",
    "c10h_annotator_mask = np.zeros((c10h_num_datapoints, c10h_num_annotators), dtype=bool) # mask of what each person annotated\n",
    "\n",
    "c10h_true_labels = np.zeros((c10h_num_datapoints, ))\n",
    "c10h_true_images = np.empty((c10h_num_datapoints, ) ,dtype=object)\n",
    "\n",
    "c10h_annotator_data = (c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c869ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotator labels as numpy array (N=labels, M=annotators)\n",
    "for annotator_id in range(c10h_num_annotators):\n",
    "    adf = c10h_df[c10h_df.annotator_id == annotator_id] # 200 annotations per annotator\n",
    "    annotations_idx = adf['cifar10_test_test_idx'].values\n",
    "    annotations = adf['chosen_label'].values\n",
    "    errors = adf['correct_guess'].values\n",
    "    \n",
    "    c10h_labels[annotations_idx, annotator_id] = annotations\n",
    "    c10h_labels_error_mask[annotations_idx, annotator_id] = errors\n",
    "    c10h_annotator_mask[annotations_idx, annotator_id] = True\n",
    "\n",
    "# get true labels as numpy array (N = true labels,)\n",
    "idx_to_label = \\\n",
    "[(idx,label,image) for idx,label,image in zip(c10h_df['cifar10_test_test_idx'],c10h_df['true_label'],c10h_df['image_filename'])]\n",
    "idx_to_label = list(set(idx_to_label))\n",
    "\n",
    "idx = [idx_to_label[0] for idx_to_label in idx_to_label]\n",
    "true_label = [idx_to_label[1] for idx_to_label in idx_to_label]\n",
    "htrue_image = [idx_to_label[2] for idx_to_label in idx_to_label]\n",
    "\n",
    "c10h_true_labels[idx] = true_label\n",
    "c10h_true_images[idx] = htrue_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a8652",
   "metadata": {},
   "source": [
    "#### Helper functions for cifar10h dataset sampling and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of individual annotators on the points they labeled\n",
    "def plt_annotator_accuracy(labels_error_mask, annotator_mask):\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size_small\n",
    "    \n",
    "    annotator_accuracy = labels_error_mask.sum(axis=0) / annotator_mask.sum(axis=0)\n",
    "    plt.boxplot(annotator_accuracy, )\n",
    "    plt.show()\n",
    "\n",
    "    df_describe = pd.DataFrame(annotator_accuracy, columns=['score'])\n",
    "    return df_describe\n",
    "\n",
    "# Returns sample labels/error_mask/annotator_mask where x_drop, y_drop are idxs that are dropped\n",
    "def get_sample_labels(x_drop, y_drop, labels, labels_error_mask, annotator_mask):\n",
    "    s_annotator_mask = annotator_mask.copy()\n",
    "    s_annotator_mask[(x_drop,y_drop)] = 0\n",
    "    s_labels_error_mask = s_annotator_mask & labels_error_mask\n",
    "    s_labels = labels.copy()\n",
    "    np.copyto(s_labels, np.nan, where=s_annotator_mask)\n",
    "    print('Total idxs dropped: ', annotator_mask.sum() - s_annotator_mask.sum())\n",
    "    return s_labels, s_labels_error_mask, s_annotator_mask\n",
    "\n",
    "# Returns a list of labeled indices to drop (random percent_dropped% of all labels)\n",
    "def get_random_drop(c10h_annotator_mask, percent_dropped=0.4):\n",
    "    x,y = np.where( c10h_annotator_mask == 1)\n",
    "    drop_idx = np.random.choice(np.arange(len(x)), int(len(x)*percent_dropped), replace=False)\n",
    "    x_drop = x[drop_idx]\n",
    "    y_drop = y[drop_idx]\n",
    "    return x_drop, y_drop\n",
    "\n",
    "# Returns a list of labeled indices to drop (random length per row)\n",
    "def get_random_drop_per_row(c10h_annotator_mask):\n",
    "    x,y = np.where(c10h_annotator_mask == 1)\n",
    "    idx_df = pd.DataFrame(zip(x,y),columns=['x','y'])\n",
    "    for x_idx in range(idx_df['x'].max()+1):\n",
    "        num_drop = np.random.randint(len(idx_df[idx_df['x'] == x_idx]))\n",
    "        idx_df = idx_df.drop(idx_df[idx_df['x'] == x_idx].sample(num_drop).index)\n",
    "    x_drop = idx_df['x'].values\n",
    "    y_drop = idx_df['y'].values\n",
    "    return x_drop, y_drop\n",
    "\n",
    "# Returns a list of labeled indices to drop (random percent_dropped% of labels per annotator)\n",
    "def get_random_drop_per_annotator(c10h_annotator_mask, percent_dropped=0.4):\n",
    "    rows_dropped = int(200 * percent_dropped)\n",
    "    print('Total rows dropped per annotator: ', rows_dropped)\n",
    "    x,y = np.where( c10h_annotator_mask == 1)\n",
    "    df_delete = pd.DataFrame(zip(x,y),columns=['x','y'])\n",
    "    df_keep = df_delete.drop(df_delete.groupby('y').sample(n=200 - rows_dropped).index)\n",
    "    x_drop,y_drop = df_keep['x'].values, df_keep['y'].values\n",
    "    return x_drop, y_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d4f11f",
   "metadata": {},
   "source": [
    "#### Helper functions for getting and analyzing consensus labels\n",
    "- consensus_labels are determined based on the individual labels of all annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines the consensus_labels\n",
    "def get_consensus_labels(labels_multiannotator, pred_probs):\n",
    "    mode_labels_multiannotator = labels_multiannotator.mode(axis=1)\n",
    "    consensus_labels = []\n",
    "    for i in range(len(mode_labels_multiannotator)):\n",
    "        consensus_labels.append( int(mode_labels_multiannotator.iloc[i][pred_probs[i][mode_labels_multiannotator.iloc[i].dropna().astype(int).to_numpy()\n",
    "                    ].argmax()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    return np.array(consensus_labels)\n",
    "\n",
    "# Get consensus labels and calculate accuracy, precision, recall, f1\n",
    "def get_consensus_label_accuracy(labels, pred_probs, true_labels):\n",
    "    labels_df = pd.DataFrame(labels)\n",
    "    consensus_labels = get_consensus_labels(labels_df, pred_probs)\n",
    "    consensus_labels_accuracy = (true_labels == consensus_labels).sum() / 10000\n",
    "    print('Consensus label accuracy: ', consensus_labels_accuracy)\n",
    "    print('\\nPer class scores:')\n",
    "    p, r, f1, _ = precision_recall_fscore_support(true_labels, consensus_labels)\n",
    "    results_df = pd.DataFrame(zip(p,r,f1), columns=['precision', 'recall', 'f1'])\n",
    "    return results_df\n",
    "\n",
    "# Plots the distribution of annotator agreement for correct/incorrect labels\n",
    "def plot_labels_multiannotator(labels, true_labels, pred_probs_multiannotator=None):\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size_small\n",
    "    \n",
    "    labels_multiannotator = pd.DataFrame(labels)\n",
    "    \n",
    "    if pred_probs_multiannotator is None:\n",
    "        pred_counts = labels.sum(axis=1)\n",
    "        pred_probs_multiannotator = labels / pred_counts[:,np.newaxis]\n",
    "\n",
    "    consensus_labels = get_consensus_labels(labels_multiannotator, pred_probs_multiannotator)\n",
    "    consensus_labels_tile = np.repeat(consensus_labels[:,np.newaxis], labels_multiannotator.shape[1], axis=1)\n",
    "    c10h_num_annotators_per_ex = np.count_nonzero(~np.isnan(labels), axis=1)\n",
    "    annotator_agreement = (labels_multiannotator == consensus_labels_tile) # Number of annotators matches consensus\n",
    "    annotator_agreement = annotator_agreement.sum(axis=1)\n",
    "    bin_consensus = (true_labels == consensus_labels) + 0\n",
    "    consensus_accuracy = pd.DataFrame(zip(annotator_agreement,bin_consensus), columns=['annotator_agreement','bin_consense'])\n",
    "    _ = consensus_accuracy.boxplot(by=['bin_consense'], figsize=(7,7))\n",
    "    consensus_accuracy = consensus_accuracy.groupby('bin_consense')[['annotator_agreement']].sum().reset_index()\n",
    "\n",
    "    return consensus_accuracy\n",
    "\n",
    "# Returns spearman correlation given two scores, checks for nans\n",
    "def get_spearman_correlation(x, y):\n",
    "    num_nans_x = np.sum(np.isnan(x))\n",
    "    num_nans_y = np.sum(np.isnan(y))\n",
    "    \n",
    "    if num_nans_x > 0:\n",
    "        x = np.nan_to_num(x)\n",
    "        print('First param contains nans. Replacing', num_nans_x, 'nans with 0')\n",
    "    if num_nans_y > 0:\n",
    "        y = np.nan_to_num(y)\n",
    "        print('First param contains nans. Replacing', num_nans_y, 'nans with 0')\n",
    "\n",
    "    return stats.spearmanr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e059999",
   "metadata": {},
   "source": [
    "## Load Cifar10 test and model output data\n",
    "- model can be trained/predictions can be generated with the ``model_train_pred.ipynb`` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries to map to display name\n",
    "method_adjust_pred_probs_display_dict = {\n",
    "    \"self_confidence-False\": \"Self Confidence\",\n",
    "    \"self_confidence-True\": \"Adjusted Self Confidence\",\n",
    "    \"normalized_margin-False\": \"Normalized Margin\",\n",
    "    \"normalized_margin-True\": \"Adjusted Normalized Margin\",\n",
    "    \"confidence_weighted_entropy-False\": \"Confidence Weighted Entropy\",\n",
    "    \"entropy-False\": \"Entropy\",\n",
    "    \"least_confidence-False\": \"Least Confidence\",\n",
    "}\n",
    "\n",
    "model_display_name_dict = {\"resnet18\": \"ResNet-18\",}\n",
    "\n",
    "# args to pass to get_label_quality_scores()\n",
    "score_params = \\\n",
    "[\n",
    "    (\"self_confidence\", False),\n",
    "    (\"self_confidence\", True),\n",
    "    (\"normalized_margin\", False),\n",
    "    (\"normalized_margin\", True),\n",
    "    (\"confidence_weighted_entropy\", False)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb5a26",
   "metadata": {},
   "source": [
    "#### Helper functions to match label indices, calculate and plot model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9460e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches indices of model trained data to cifar10h annotation dataset\n",
    "def match_label_indices(numpy_out_folder, c10h_annotator_data, model):\n",
    "    # read numpy files from model_train_pred\n",
    "    pred_probs = np.load(numpy_out_folder + \"test_pred_probs_\" + model + \".npy\")\n",
    "    pred_labels = np.load(numpy_out_folder + \"test_preds_\" + model + \".npy\")\n",
    "    true_labels = np.load(numpy_out_folder + \"test_labels_\" + model + \".npy\")\n",
    "    images = np.load(numpy_out_folder + \"test_images_\" + model + \".npy\", allow_pickle=True)\n",
    "    idxs = [int(image.split('/')[-1][-8:-4]) for image in images]\n",
    "    \n",
    "    # set all cifar10h annotator data to the correct indexing\n",
    "    c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images = c10h_annotator_data\n",
    "    \n",
    "    c10h_true_labels = c10h_true_labels[idxs]\n",
    "    c10h_true_images = c10h_true_images[idxs]\n",
    "    c10h_labels = c10h_labels[idxs]\n",
    "    c10h_labels_error_mask = c10h_labels_error_mask[idxs]\n",
    "    c10h_annotator_mask =  c10h_annotator_mask[idxs]\n",
    "    \n",
    "    # check cifar10h ordering matches what our model predicted on\n",
    "    assert np.array_equal(c10h_true_labels, true_labels)\n",
    " \n",
    "    c10h_annotator_data = (c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images)\n",
    "    c10_model_data = (pred_probs, pred_labels, true_labels, images)\n",
    "    \n",
    "    return c10_model_data, c10h_annotator_data\n",
    "    \n",
    "def benchmark_results(c10_model_data, c10h_annotator_data, score_params, model):\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size_big\n",
    "    \n",
    "    c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images = c10h_annotator_data\n",
    "    pred_probs, pred_labels, true_labels, images = c10_model_data\n",
    "\n",
    "    assert np.array_equal(c10h_true_labels, true_labels) # check cifar10h ordering matches what our model predicted on\n",
    "    \n",
    "    # create boolean mask of label errors\n",
    "    labels = pred_labels # labels can change to annotator labels!!\n",
    "    label_errors_target = labels != true_labels\n",
    "    \n",
    "    results = []\n",
    "    results_list = []\n",
    "\n",
    "    for score_param in score_params:\n",
    "        method, adjust_pred_probs = score_param\n",
    "        # compute scores\n",
    "        label_quality_scores = get_label_quality_scores(labels=labels, pred_probs=pred_probs, method=method, adjust_pred_probs=adjust_pred_probs)\n",
    "        # compute accuracy of detecting label errors\n",
    "        auroc = roc_auc_score(label_errors_target, 1 - label_quality_scores)\n",
    "        # compute Lift@K evaluation metric\n",
    "        lift_at_k_dict = {}\n",
    "        for k in range(1000, 11000, 1000):\n",
    "            lift_at_k_dict[f\"lift_at_{k}\"] = lift_at_k(label_errors_target, 1 - label_quality_scores, k=k)\n",
    "        # save results\n",
    "        results = {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"model\": model,\n",
    "            \"noise_config\": \"nan\",\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"auroc\": auroc\n",
    "        }\n",
    "        # add the lift at k metrics\n",
    "        results.update(lift_at_k_dict)\n",
    "        # save results\n",
    "        results_list.append(results)\n",
    "\n",
    "        # compute precision-recall curve using label quality scores\n",
    "        precision, recall, thresholds = precision_recall_curve(label_errors_target, 1 - label_quality_scores)\n",
    "\n",
    "        # compute au-roc curve using label quality scores\n",
    "        fpr, tpr, thresholds = roc_curve(label_errors_target,  1 - label_quality_scores)\n",
    "\n",
    "        precision_recall_curve_results = {\n",
    "            \"method\": method,\n",
    "            \"adjust_pred_probs\": adjust_pred_probs,\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"model\": model,\n",
    "            \"label_quality_scores\": label_quality_scores,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"thresholds\": thresholds\n",
    "        }\n",
    "        \n",
    "        # plot prc\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(recall, precision, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "        plt.xlabel(\"Recall\", fontsize=14)\n",
    "        plt.ylabel(\"Precision\", fontsize=14)\n",
    "        plt.title(\"Precision-Recall Curve: Label Error Detection on CIFAR-10h \\n Model: resnet-18\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(fpr, tpr, label=f\"{method}-{str(adjust_pred_probs)}\")\n",
    "        plt.xlabel(\"False Positive Rate\", fontsize=14)\n",
    "        plt.ylabel(\"True Positive Rate\", fontsize=14)\n",
    "        plt.title(\"AU ROC Curve: Label Error Detection on CIFAR-10h \\n Model: resnet-18\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.legend()\n",
    "        \n",
    "    return results_list, precision_recall_curve_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830368a0",
   "metadata": {},
   "source": [
    "### Load cifar10test and model output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "numpy_out_folder = './data/model_data_070622/' # folder containing model train data\n",
    "model = \"resnet18\" # can also be: \"swin_base_patch4_window7_224\"\n",
    "c10_model_data, c10h_annotator_data = match_label_indices(numpy_out_folder, c10h_annotator_data, model)\n",
    "\n",
    "# unpack fixed indices\n",
    "c10h_labels, c10h_labels_error_mask, c10h_annotator_mask, c10h_true_labels, c10h_true_images = c10h_annotator_data\n",
    "pred_probs, pred_labels, true_labels, images = c10_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ec7fe",
   "metadata": {},
   "source": [
    "### Optional: Drop out annotators from data\n",
    "- Use either ``get_random_drop_per_annotator()``, ``get_random_drop_per_row()``, ``get_random_drop()``\n",
    "- TODO: ``get_random_drop_per_row()`` very slow, needs to be optimized with lambda\n",
    "- TODO: Test ``get_random_drop_per_row()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ddc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_drop, y_drop = get_random_drop_per_row(c10h_annotator_mask)\n",
    "# c10h_labels, c10h_labels_error_mask, c10h_annotator_mask = \\\n",
    "#                     get_sample_labels(x_drop, y_drop, c10h_labels, c10h_labels_error_mask, c10h_annotator_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df7cf6d",
   "metadata": {},
   "source": [
    "# 2. Benchmark Evaluation -- experiment as needed\n",
    "\n",
    "**Major Benchmarks**\n",
    "- How good is consensus quality-score for any given consensus\n",
    "    - Plots: auroc, auprc\n",
    "    - Scores: auroc\n",
    "- How accurate are consensus labels: accuracy\n",
    "    - Overall: accuracy\n",
    "    - Per-class: precision,recall,f1 scores\n",
    "- How good are annotator-quality scores\n",
    "    - Distribution of individual annotator accuracy vs ground truth\n",
    "    - Spearman correlation between annotator scores vs annotator accuracy\n",
    "\n",
    "**Side Benchmarks**\n",
    "- Annotator Agreement Consensus vs. Label Accuracy\n",
    "- Annotator vs Model Accuracy\n",
    "- Label quality scores distribution\n",
    "- Worst Class per Annotator distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3fb618",
   "metadata": {},
   "source": [
    "## How good is consensus quality-score for any given consensus\n",
    "#### AUROC vs AUPRC curves\n",
    "- ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability threshold\n",
    "- Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n",
    "- Precision-Recall curves: imbalanced datasets (more sensitive to positive class), ROC curves: balanced datasets\n",
    "- Data behavior in curves is relatively similar (i.e. well performing label will perform well in both curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702132b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list, precision_recall_curve_results = benchmark_results(c10_model_data, c10h_annotator_data, score_params, \"resnet18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db55d8",
   "metadata": {},
   "source": [
    "#### AUROC score per method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14a73e",
   "metadata": {},
   "source": [
    "## How accurate are consensus labels\n",
    "#### Overall accuracy and per-class precision, recall, f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_consensus_label_accuracy(c10h_labels, pred_probs, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffe4cf",
   "metadata": {},
   "source": [
    "## How good are annotator-quality scores \n",
    "#### Distribution of individual annotator accuracy vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c616c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_accuracy_df = plt_annotator_accuracy(c10h_labels_error_mask,  c10h_annotator_mask)\n",
    "annotator_accuracy_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292acf9",
   "metadata": {},
   "source": [
    "#### Spearman Correlation\n",
    "- Nonparametric measure of the monotonicity of the relationship between two datasets\n",
    "- Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.\n",
    "- The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. The p-values are not entirely reliable\n",
    "- TODO: figure out why spearman correlation is 0 when we randomly remove random number of datapoints per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_quality_multiannotator, multiannotator_stats = get_label_quality_multiannotator(pd.DataFrame(c10h_labels), pred_probs, return_annotator_stats = True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_score = multiannotator_stats['overall_quality'].values\n",
    "accuracy = annotator_accuracy_df['score'].values\n",
    "get_spearman_correlation(quality_score, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3860339",
   "metadata": {},
   "source": [
    "## Side Benchmarks\n",
    "#### Annotator Agreement Consensus vs. Label Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels_multiannotator(c10h_labels, c10h_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd1f45",
   "metadata": {},
   "source": [
    "#### Annotator vs Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc25b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much the consensus labels differ from\n",
    "c10h_labels_df = pd.DataFrame(c10h_labels)\n",
    "consensus_labels = get_consensus_labels(c10h_labels_df, pred_probs)\n",
    "\n",
    "print('Probability annotators alone correctly predict labels: ', (true_labels == consensus_labels).sum() / 10000)\n",
    "model_pred_labels = np.argmax(pred_probs, axis=1) # true labels == to what the model is likeley to predict\n",
    "print('Probability model alone correctly predicts labels: ', (model_pred_labels == true_labels).sum() / 10000) # suggests model is x% likeley to predict with the consensus\n",
    "print('Similar prediction between model preds and consensus_labels: ', (model_pred_labels == consensus_labels).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deba0e3",
   "metadata": {},
   "source": [
    "#### Label quality scores distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39726d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_quality_scores = get_label_quality_scores(consensus_labels, pred_probs)\n",
    "pd.DataFrame(label_quality_scores, columns=['']).plot.hist(bins=30,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aaffb4",
   "metadata": {},
   "source": [
    "#### Worst Class Per Annotator Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9082ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiannotator_stats = get_multiannotator_stats(c10h_labels_df, pred_probs, consensus_labels, 'agreement')\n",
    "multiannotator_stats['worst_class'].plot.hist(bins=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee9120",
   "metadata": {},
   "source": [
    "# 3. Label Issue Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f95c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0:\"airplane\", \n",
    "           1:\"automobile\", \n",
    "           2:\"bird\", \n",
    "           3:\"cat\", \n",
    "           4:\"deer\",\n",
    "           5:\"dog\", \n",
    "           6:\"frog\", \n",
    "           7:\"horse\", \n",
    "           8:\"ship\", \n",
    "           9:\"truck\"}\n",
    "\n",
    "def get_label_issues(labels, pred_probs, true_labels, images):\n",
    "    label_issues = find_label_issues(labels=labels,\n",
    "                        pred_probs=pred_probs,\n",
    "                        return_indices_ranked_by='self_confidence',\n",
    "                        )\n",
    "    issue_consensus_labels = labels[label_issues]\n",
    "    issue_images = images[label_issues]\n",
    "    issue_true_labels = true_labels[label_issues]\n",
    "    issue_is_issue = (issue_consensus_labels != issue_true_labels) + 0\n",
    "    issue_real_image_paths = [path + '/' + '/'.join(image.split('/')[-5:]) for image in issue_images]\n",
    "\n",
    "    print('Number of label issues detected: ', len(label_issues))\n",
    "    print('Number of true label issues: ', true_labels.shape[0] - np.sum(labels == true_labels))\n",
    "    print('Number of true label issues detected: ', np.sum(issue_is_issue))\n",
    "    \n",
    "    issues_df = pd.DataFrame(zip(issue_consensus_labels, issue_true_labels, issue_is_issue, issue_real_image_paths),\n",
    "            columns = ['label','true_label','is_issue','image_png'])\n",
    "    return issues_df\n",
    "\n",
    "def visualize_label_issues(issues_df, classes, scale=100):\n",
    "    print('Visualizing', len(issues_df), 'issues\\n')\n",
    "    \n",
    "    listOfImageNames = issues_df['image_png'].values\n",
    "\n",
    "    for index, row in issues_df.iterrows():\n",
    "        print('Correctly identified: ', bool(row['is_issue']), '\\nGiven label: ', classes[row['label']], '\\nTrue label: ', classes[row['true_label']],)\n",
    "        image = Image(filename=row['image_png'])\n",
    "        display(Image(filename=row['image_png'], width=scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd99baf",
   "metadata": {},
   "source": [
    "#### Run find_label_issues on the consensus labels and visualize label issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much the consensus label issues differ from true labels\n",
    "c10h_labels_df = pd.DataFrame(c10h_labels)\n",
    "consensus_labels = get_consensus_labels(c10h_labels_df, pred_probs)\n",
    "issues_df = get_label_issues(consensus_labels, pred_probs, true_labels, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_issues_df = issues_df[issues_df['is_issue'] == 1] # get a df of correctly identified true issues\n",
    "# visualize_label_issues(issues_df, classes)\n",
    "visualize_label_issues(true_issues_df, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da87ed",
   "metadata": {},
   "source": [
    "#### Run find_label_issues on an individual annotator's labels and visualize label issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_accuracy = c10h_labels_error_mask.sum(axis=0) /  c10h_annotator_mask.sum(axis=0)\n",
    "worst_annotator = np.argmin(annotator_accuracy)\n",
    "best_annotator = np.argmax(annotator_accuracy)\n",
    "print('worst annotator: ', worst_annotator, 'accuracy: ', annotator_accuracy[worst_annotator])\n",
    "print('best annotator: ', best_annotator, 'accuracy: ', annotator_accuracy[best_annotator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542cff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_id = 1 # worst = 2561, best = 1957\n",
    "\n",
    "a_c10h_labels = c10h_labels[:,annotator_id]\n",
    "a_c10h_annotator_mask =  c10h_annotator_mask[:,annotator_id]\n",
    "a_c10h_labels_error_mask = c10h_labels_error_mask[:,annotator_id]\n",
    "\n",
    "a_labels = true_labels.copy()\n",
    "a_labels[a_c10h_annotator_mask] = a_c10h_labels[a_c10h_annotator_mask]\n",
    "\n",
    "print('Annotator accuracy: ', np.sum(a_labels[a_c10h_annotator_mask] == true_labels[a_c10h_annotator_mask]) / a_c10h_annotator_mask.sum())\n",
    "print('Annotator accuracy: ', annotator_accuracy[annotator_id])\n",
    "print('Num correctly labeled points for annotator ', annotator_id, ': ', np.sum(a_labels == true_labels))\n",
    "print('Annotator + True label accuracy: ', np.sum(a_labels == true_labels) / len(a_labels))\n",
    "\n",
    "issues_df = get_label_issues(a_labels, pred_probs, true_labels, images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
