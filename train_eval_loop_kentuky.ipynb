{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264fb0bf",
   "metadata": {},
   "source": [
    "# This notebook runs a train and eval loop on models with improving consensus labels over each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36123b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep ground truth labels as self contained as possible! define #ground truth not actually needed\n",
    "# get and save different consensus labels in a dictionary cleanly\n",
    "# compute accuracy but make it clear that is not something the user will want to do\n",
    "\n",
    "# label some data\n",
    "# get improved consensus quality scores and more data\n",
    "# train notebook (active learning with a twist because you can get datapoint for something you've already labeled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346fb764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulyana/virtual/multi/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from utils.model_training import train_models\n",
    "from utils.model_training import sum_xval_folds\n",
    "from utils.data_loading import get_annotator_labels\n",
    "from utils.data_loading import drop_and_distribute\n",
    "from utils.data_loading import get_ground_truth_data_matched\n",
    "from utils.data_loading import get_and_save_improved_consensus_label\n",
    "from utils.data_loading import get_and_save_consensus_labels\n",
    "from cleanlab.multiannotator import get_label_quality_multiannotator # only in hui wen directory\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a9a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./data/experiments/experiment_1660859529  Created \n",
      "Experiment saved in ./data/experiments/experiment_1660859529\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now() # current date and time\n",
    "experiment_folder = \"experiment_\" + str(int(now.timestamp()))\n",
    "dirName = './data/experiments/' + experiment_folder\n",
    "\n",
    "if not os.path.exists(dirName):\n",
    "    os.makedirs(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , dirName ,  \" already exists\")\n",
    "\n",
    "print(f'Experiment saved in {dirName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a6e28",
   "metadata": {},
   "source": [
    "## Dropout data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f2b53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all examples have at least 1 annotator\n",
      "num_worst_annotators_selected 457\n",
      "Total idxs dropped:  488003.0\n",
      "Make sure 10.0 <= 5 and 1.0 > 0: \n",
      "Total idxs dropped:  8139.0\n",
      "Make sure 9.0 <= 5 and 1.0 > 0: \n"
     ]
    }
   ],
   "source": [
    "# Get cifar10h dataset and dropout information from it\n",
    "cifar10_infolder = './data/cifar-10h/cifar10h-raw.csv' #c10h raw data folder\n",
    "max_annotations = 5\n",
    "\n",
    "c10h_labels, c10h_true_labels, c10h_true_images = get_annotator_labels(cifar10_infolder)\n",
    "c10h_labels = drop_and_distribute(c10h_labels, c10h_true_labels, max_annotations)\n",
    "\n",
    "# save c10h_results\n",
    "cifar10_labels_folder = f\"{dirName}/todelete_c10h_labels_range_{max_annotations}.npy\"\n",
    "cifar10_true_labels_folder = f\"{dirName}/todelete_c10h_true_labels_range_{max_annotations}.npy\"\n",
    "np.save(cifar10_labels_folder, c10h_labels)\n",
    "np.save(cifar10_true_labels_folder, c10h_true_labels)\n",
    "\n",
    "# Generate and save original consensus labels\n",
    "consensus_outfolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_0.csv' #output folder for consensus labels\n",
    "consensus_labels = get_and_save_consensus_labels(c10h_labels, c10h_true_labels, consensus_outfolder)\n",
    "\n",
    "# Generate label quality of each annotator\n",
    "label_quality_multiannotator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce19d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10_infolder = './data/cifar-10h/cifar10h-raw.csv' #c10h raw data folder\n",
    "# max_annotations = 5\n",
    "\n",
    "# c10h_labels, c10h_true_labels, c10h_true_images = get_annotator_labels(cifar10_infolder)\n",
    "# np.save('./benchmark_data/c10h_true_labels.npy', c10h_true_labels)\n",
    "# np.save('./benchmark_data/c10h_true_images.npy', c10h_true_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ccc8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_locs = [f\"cifar10_test/{'/'.join(image.split('/')[-2:])}\" for image in consensus_labels[\"image\"]]\n",
    "np.save('c10h_image_paths.npy', image_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea78160e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cifar10_test/cat/test_batch_index_0000.png',\n",
       " 'cifar10_test/ship/test_batch_index_0001.png',\n",
       " 'cifar10_test/ship/test_batch_index_0002.png']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image_locs = ['/'.split(image) for image in consensus_labels[\"image\"]]\n",
    "image_locs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3ebe3",
   "metadata": {},
   "source": [
    "## Train models through loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd0ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consensus labels and train model on them\n",
    "models = [\n",
    "    \"resnet18\",\n",
    "    \"swin_base_patch4_window7_224\"\n",
    "]\n",
    "\n",
    "train_args = {\n",
    "    \"num_cv_folds\": 5, \n",
    "    \"verbose\": 1, \n",
    "    \"epochs\": 100, \n",
    "    \"holdout_frac\": 0.2, \n",
    "    \"time_limit\": 21600, \n",
    "    \"random_state\": 123\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through and retrain model on better pred-probs\n",
    "# NUM_MODEL_RETRAINS = 7\n",
    "\n",
    "# # check if new consensus labels exist in the set of prior consensus labels and stop if yes (cycle maybe)\n",
    "# for i in range(NUM_MODEL_RETRAINS):\n",
    "#     for model in models:\n",
    "#         # Get folders\n",
    "#         consensus_infolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_0.csv' \n",
    "# #         if i == 0:\n",
    "# #             consensus_infolder = consensus_outfolder\n",
    "# #         else:\n",
    "# #             consensus_infolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i-1}_{model}.csv'\n",
    "#         model_results_folder = f'{dirName}/todelete_cifar10_consensus_range_{max_annotations}_{i}' # + [model_type]\n",
    "#         consensus_outfolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i}_{model}.csv'\n",
    "        \n",
    "#         df = pd.read_csv(consensus_infolder)\n",
    "#         print(df.head())\n",
    "#         print(f'--INFO {i}_{model}--')\n",
    "#         print('Loading consensus from', consensus_infolder)\n",
    "#         print('Saving consensus to', consensus_outfolder)\n",
    "#         print('Saving model results to', model_results_folder)\n",
    "#         print('---------------------')\n",
    "        \n",
    "        \n",
    "# #         # Train model\n",
    "# #         train_models([model], consensus_infolder, model_results_folder, **train_args)\n",
    "# #         pred_probs, labels , true_labels, images = sum_xval_folds([model], model_results_folder, **train_args)\n",
    "        \n",
    "# #         # Get label quality multiannotator\n",
    "# #         label_quality_multiannotator = get_label_quality_multiannotator(c10h_labels,pred_probs,verbose=False)\n",
    "\n",
    "# #         # Generate and save consensus labels\n",
    "# #         _ = get_and_save_improved_consensus_label(label_quality_multiannotator, c10h_true_labels, consensus_outfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and retrain model on better pred-probs\n",
    "NUM_MODEL_RETRAINS = 7\n",
    "\n",
    "# check if new consensus labels exist in the set of prior consensus labels and stop if yes (cycle maybe)\n",
    "for i in range(NUM_MODEL_RETRAINS):\n",
    "    for model in models:\n",
    "        # Get folders\n",
    "        if i == 0:\n",
    "            consensus_infolder = consensus_outfolder\n",
    "        else:\n",
    "            consensus_infolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i-1}_{model}.csv'\n",
    "        model_results_folder = f'{dirName}/todelete_cifar10_consensus_range_{max_annotations}_{i}' # + [model_type]\n",
    "        consensus_outfolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i}_{model}.csv'\n",
    "        \n",
    "        print(f'--INFO {i}_{model}--')\n",
    "        print('Loading consensus from', consensus_infolder)\n",
    "        print('Saving consensus to', consensus_outfolder)\n",
    "        print('Saving model results to', model_results_folder)\n",
    "        print('---------------------')\n",
    "        \n",
    "        \n",
    "        # Train model\n",
    "        train_models([model], consensus_infolder, model_results_folder, **train_args)\n",
    "        pred_probs, labels , true_labels, images = sum_xval_folds([model], model_results_folder, **train_args)\n",
    "        \n",
    "        # Get label quality multiannotator\n",
    "        label_quality_multiannotator = get_label_quality_multiannotator(c10h_labels,pred_probs,verbose=False)\n",
    "\n",
    "        # Generate and save consensus labels\n",
    "        _ = get_and_save_improved_consensus_label(label_quality_multiannotator, c10h_true_labels, consensus_outfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e62b2",
   "metadata": {},
   "source": [
    "## Compute accuracy of model based on Accuracy (labels vs true labels) by itter after folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_noisy_vs_true_labels = (consensus_labels['label'].values == c10h_true_labels).mean()\n",
    "print(f\"Accuracy ORIGINAL (consensus labels vs true labels): {acc_noisy_vs_true_labels}\\n\")\n",
    "\n",
    "for model in models:\n",
    "    for i in range(NUM_MODEL_RETRAINS):\n",
    "        \n",
    "        # Get folders\n",
    "        if i == 0:\n",
    "            consensus_infolder = consensus_outfolder\n",
    "        else:\n",
    "            consensus_infolder = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i-1}_{model}.csv'\n",
    "        model_results_folder = f'{dirName}/todelete_cifar10_consensus_range_{max_annotations}_{i}' # + [model_type]\n",
    "    \n",
    "        print(f'--{model} iter{i}--')\n",
    "        \n",
    "        out_subfolder = f\"{model_results_folder}_{model}/\"\n",
    "        pred_probs = np.load(out_subfolder + \"pred_probs.npy\")\n",
    "        labels = np.load(out_subfolder + \"labels.npy\") # remember that this is the noisy labels (s)\n",
    "        images = np.load(out_subfolder + \"images.npy\", allow_pickle=True)\n",
    "        true_labels = np.load(out_subfolder + \"true_labels.npy\")\n",
    "\n",
    "        # check the accuracy\n",
    "        acc_labels = (pred_probs.argmax(axis=1) == labels).mean() # noisy labels (s)\n",
    "        acc_true_labels = (pred_probs.argmax(axis=1) == true_labels).mean() # true labels (y)    \n",
    "        acc_noisy_vs_true_labels = (labels == true_labels).mean()\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Accuracy (argmax pred vs labels)                 : {acc_labels}\")\n",
    "        print(f\"  Accuracy (argmax pred vs true labels)            : {acc_true_labels}\")\n",
    "        print(f\"  Accuracy (consensus labels vs true labels)       : {acc_noisy_vs_true_labels}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_mask = np.logical_not(np.isnan(c10h_labels))\n",
    "pd.DataFrame(annotator_mask.sum(axis=0)).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(annotator_mask.sum(axis=1)).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42eb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "consensus_file = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_0.csv'\n",
    "clabels = pd.read_csv(consensus_file)['label'].values\n",
    "print('true: ', c10h_true_labels[:20])\n",
    "print('cons: ', clabels[:20])\n",
    "accuracy = np.mean(labels == c10h_true_labels).mean()\n",
    "print(f\"Accuracy ORIGINAL (consensus labels vs true labels): {accuracy}\\n\")\n",
    "\n",
    "model = \"resnet18\"\n",
    "\n",
    "for i in range(NUM_MODEL_RETRAINS):\n",
    "    model_results_folder = f'{dirName}/todelete_cifar10_consensus_range_{max_annotations}_{i}' # + [model_type]\n",
    "    out_subfolder = f\"{model_results_folder}_{model}/\"\n",
    "\n",
    "    true_labels = np.load(out_subfolder + \"true_labels.npy\")\n",
    "    print('true: ', true_labels[:20])\n",
    "    print('cons: ', clabels[:20])\n",
    "    x = np.where(true_labels != clabels.astype(int))[0]\n",
    "    print(true_labels[x][:20], )\n",
    "    print((10000 - len(x) ) /10000)\n",
    "    \n",
    "    if not (true_labels == clabels.astype(int)).all():\n",
    "        print('something is wrong!')\n",
    "    \n",
    "    consensus_file = f'{dirName}/todelete_cifar10_test_consensus_dataset_range_{max_annotations}_{i}_{model}.csv'\n",
    "    clabels = pd.read_csv(consensus_file)['label'].values\n",
    "    accuracy = np.mean(clabels == c10h_true_labels).mean()\n",
    "    print(f\"{i}: Accuracy (consensus labels vs true labels): {accuracy}\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.multiannotator import get_majority_vote_label\n",
    "consensus_labels_true = np.load('benchmark_data/c10h_labels_worst_20_coin20.npy')\n",
    "consensus_labels_true.shape\n",
    "\n",
    "consensus_labels_true = get_majority_vote_label(pd.DataFrame(consensus_labels_true), pred_probs=None)\n",
    "consensus_labels_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e29729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(consensus_labels_true == c10h_true_labels)\n",
    "print(f\"Accuracy ORIGINAL (consensus labels vs true labels): {accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_labels = get_and_save_consensus_labels(c10h_labels, c10h_true_labels, consensus_outfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25162dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "def get_image_paths(images, image_data_folder):\n",
    "    classes = {\"airplane\": 0, \n",
    "           \"automobile\": 1, \n",
    "           \"bird\": 2, \n",
    "           \"cat\": 3, \n",
    "           \"deer\": 4, \n",
    "           \"dog\": 5, \n",
    "           \"frog\": 6, \n",
    "           \"horse\": 7, \n",
    "           \"ship\": 8, \n",
    "           \"truck\": 9}\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "print(path)\n",
    "print('images[0]: domestic_cat_s_000907.png')\n",
    "\n",
    "image_data_folder = 'data/cifar10/test' # datafolder ending is split of test and train\n",
    "\n",
    "# image_locs = [f\"{path}/{image_data_folder}/{get_animal(im)}/test_batch_index_{get_idx(im)}\" for im in images]\n",
    "consensus_data = './data/benchmark_data/cifar10_test_consensus_dataset_worst_20_coin20.csv'\n",
    "consensus_df = pd.read_csv(consensus_data)\n",
    "consensus_df.head()\n",
    "\n",
    "# consensus_df['image'][0]\n",
    "\n",
    "# consensus_df['image'] = \\\n",
    "# consensus_df['image'].apply(lambda x: path + '/' + '/'.join(x.split('/')[-5:]))\n",
    "# consensus_df.to_csv('./data/' + model_folder + '/cifar10_test_consensus_dataset_worst_25_coin20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "! cd data/benchmark_data/cifar10_test_consensus_dataset_worst_20_coin20.csv && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce99e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_df = pd.read_csv(consensus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af07cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_labels.iloc[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678bcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
